---
title: 'Practical 8: Bayesian hierachical models'
author: "VIBASS"
date: "July 2022"
output:
  html_vignette:
    fig_caption: yes
    number_sections: yes
    toc: yes
    fig_width: 6
    fig_height: 4
vignette: >
  %\VignetteIndexEntry{Practical 8: Bayesian hierachical models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{R2BayesX}
---

# Introduction

So far, only simple Bayesian models with conjugate priors have been considered.
As explained in previous practicals, when the posterior distribution is not
available in closed form, the Metropolis-Hastings algorithm can be used to
obtain samples from it. In general, posterior distributions are
seldom available in closed form and implementing the Metropolis-Hastings
algorithm for complex models can be tricky.

For this reason, in this practical several `R` packages to fit Bayesian
hierarchical models will be explored. The examples that have been considered
in previous practicals will be revisited, but the computational tools discussed
now can be used to deal with more complex models.

In particular, the following two software packages will be considered:


* `BayesX`

* `INLA`

`BayesX` (http://www.bayesx.org/) implements MCMC methods to obtain samples from
the joint posterior and is more conveniently used from R using the package
`R2BayesX`.

`INLA` (https://www.r-inla.org/) is based on producing (accurate) approximations to the marginal
distributions of the model parameters. Although this can be enough most of the
time, making multivariate inference with `INLA` can be difficult or impossible.
However, in many cases this is not needed and `INLA` can fit some classes of models in a
fraction of the time it takes with MCMC.

Both `R2BayesX` and `INLA` have a
very simple interface to define models using a `formula` (in the same way as with `glm` and `gam`).

While `R2BayesX` can be installed from CRAN, `INLA` is not on CRAN and needs to
be installed from a specific repository.


# `BayesX`


## Binomial model

The model that we will be using now is slightly different from the one
presented in previous practicals. In particular, instead of a Binomial we
will consider each one of the Bernoulli trials, so that data is encoded
as $0$ or $1$ and the model becomes:

$$
\begin{array}{rcl}
y_{ij} & \sim & Bi(1,\theta)\\
\textrm{logit}(\theta) & = & \alpha\\
\alpha & \sim & N(0, \tau_{\alpha})
\end{array}
$$

Note that now the probability $\theta$ is not modeled directly but that its logit
depends on a linear predictor, which is made of a single intercept term
$\alpha$. This term is assigned a (vague) Gaussian prior.

You can use the data from the proportion of red M&Ms obtained
before:

```{r eval = TRUE}
data <- data.frame(MMs = c(35, 36, 35), red = c(5, 8, 9))
```

Before fitting the model, you will need to transform the original data into binary data:

```{r}
red.bin <- rep(c(1,0), times = c(sum(data$red), 
  sum(data$MMs) - sum(data$red)))
```

As mentioned above, fitting a model with `BayesX` is similar to fitting a
model with the `glm` function:

```{r results = "hide"}
library(R2BayesX)
bayesx.bin <- bayesx(red.bin ~ 1, family = "binomial", method = "MCMC")
```

Note option `method = "MCMC"` which is needed to use MCMC to fit a Bayesian
model. `BayesX` also supports restricted maximum likelihood (REML) and
penalized least squares (PLS). Summary results can be displayed as follows:

```{r}
summary(bayesx.bin)
```


Note that the output shows the summary of the intercept term $\alpha$ and not of
$\theta$. In this simple case, we can obtain a sample from the posterior
of $\theta$ by back-transforming the samples of the intercept term, i.e.,
$\theta = \textrm{expit}(\alpha) = 1/(1 + \exp(-\alpha))$.

```{r}
bayesx.theta <- 1/(1 + exp(-attr(bayesx.bin$fixed.effects, "sample")[,1]))
summary(bayesx.theta)
```

```{r fig = TRUE}
par(mfrow = c(1, 2))

plot(bayesx.theta, type = "l", main = "MCMC samples", ylab = expression(theta))
plot(density(bayesx.theta), main = "Posterior density",
  xlab = expression(theta))
```




## Poisson model

A Poisson model has already been used for the *Game of Thrones* data set. In this case,
a slightly different model will be fitted:

$$
\begin{array}{rcl}
y_i & \sim & Po(\theta)\\
\log(\theta) & = & \alpha\\
\alpha & \sim & N(0, \tau_{\alpha})
\end{array}
$$

In BayesX, $\tau_{\alpha}$ is assigned a Gamma prior with parameters 0.001
and 0.001, but we will ignore that for now.

This is often called a *Poisson model* or *log-linear model* because the
logarithm of the mean (i.e., $\theta$) is modeled on a vector of covariates.
In this case the linear predictor is made up of a single intercept term, which
is assigned a Gaussian prior. 

To fit this model, use the data collected previously, which can be loaded with:

```{r eval = TRUE}
data <- data.frame(Us = c(25, 29, 27, 27, 25, 27, 22, 26, 27, 29, 23, 28, 25,
  24, 22, 25, 23, 29, 23, 28, 21, 29, 28, 23, 28))
Us <- data$Us
```

Fitting this model with `BayesX` is very easy:

```{r}
bayesx.pois <- bayesx(Us ~ 1, family = "poisson", method = "MCMC")
summary(bayesx.pois)
```

Note that the output shows the summary of the intercept term $\alpha$ and not of
$\theta$. In this simple case, we can obtain a sample from the posterior
of $\theta$ by back-transforming the samples of the intercept term, i.e., 
$\theta = \exp(\alpha)$.

```{r results = "hide"}
bayesx.theta <- exp(attr(bayesx.pois$fixed.effects, "sample")[,1])
```

```{r}
summary(bayesx.theta)
```

```{r fig = TRUE}
par(mfrow = c(1, 2))

plot(bayesx.theta, type = "l", main = "MCMC samples", ylab = expression(theta))
plot(density(bayesx.theta), main = "Posterior density", xlab = expression(theta))
```

## Normal model 

The final model is based on the height data collected before. The actual
model that will be considered is:


$$
\begin{array}{rcl}
y_i & \sim & N(\mu, \tau)\\
\mu & = & \alpha\\
\alpha & \sim & N(0, \tau_{\alpha})\\
\tau & \sim & Ga(0.001, 0.001)
\end{array}
$$

In this case, we have explicitly written the mean $\mu$ equal to a linear
predictor because this model can be generalised to include additive effects on
several covariates.


You can load the height data using this:

```{r eval = TRUE}
data <- data.frame(height = c(1.6, 1.62, 1.69, 1.58, 1.57, 1.57, 1.47, 1.59,
  1.59, 1.66, 1.59, 1.73, 1.61, 1.59, 1.73, 1.92, 1.74, 1.82, 1.74, 1.82, 1.89,
  1.96, 1.83, 1.81, 1.74, 1.75, 1.9, 1.69, 1.92, 1.77))
heights <- data$height
```


Similarly as in the previous examples, the model can be fitted as:

```{r results = "hide"}
bayesx.norm <- bayesx(heights ~ 1, family = "gaussian", method = "MCMC")
```

```{r}
summary(bayesx.norm)
```

In this case, mean $\mu$ is equal to $\alpha$, so there is no need
to make any transformation. However, `BayesX` reports the variance, so we
need to take the inverse of the samples in order to estimate the precision:


```{r}
bayesx.mu <- attr(bayesx.norm$fixed.effects, "sample")[,1]
bayesx.tau <- 1 / attr(bayesx.norm$variance, "sample")
```


```{r fig = TRUE}
par(mfrow = c(2, 2))

plot(bayesx.mu, type = "l", main = "MCMC samples",
  ylab = expression(mu))
plot(density(bayesx.mu), main = "Posterior density",
  xlab = expression(mu))
plot(bayesx.tau, type = "l", main = "MCMC samples",
  ylab = expression(tau))
plot(density(bayesx.tau), main = "Posterior density",
  xlab = expression(tau))
```

# `INLA`

The `R-INLA` package is an implementation of the *integrated nested
Laplace approximation* method in `R`. This method is based on obtaining accurate
approximations to the posterior marginals of the model parameters using
different numerical approximations with the Laplace method. 



Model-fitting with `R-INLA` is very similar to model-fitting with `BayesX`.
Instead of calling `bayesx()`, you will be calling `inla()`. Of course there
are important differences between these two packages but, in practice, the process of
fitting models works in a very similar way.


## Binomial model

First, a model to the M&Ms data set will be fitted:

```{r inla-available, echo = FALSE}
inla_available <- require(INLA)
```

```{r message-if-inla-not-available, include = inla_available, echo = FALSE}
if (!inla_available) {
  cat(
    "Sorry. INLA has not been installed correctly.",
    "The code will be shown but not the results.",
    "Please install INLA manually from",
    "https://www.r-inla.org/download-install",
    "and reinstall the package {vibass}."
  )
}
```


```{r eval = inla_available}
library(INLA)
inla.bin <- inla(red.bin ~ 1, family = "binomial",
  data = list(red.bin = red.bin))
summary(inla.bin)
```

The output shows summary statistics of the intercept, which is not
the proportion. We need to back-transform the output but, in this case, 
we need to back-transform the marginal distribution of the intercept (because there
are no samples from the posterior):

```{r eval = inla_available}
marg.theta <- inla.tmarginal(function(x) { 1 / (1 + exp(-x))}, 
  inla.bin$marginals.fixed[[1]])
```
Here, function `inla.tmarginal` will take the posterior marginal of a
parameter and compute the marginal of a transformation of this parameter.
In this case, we have the marginal of $\alpha$ and we compute the marginal
of the transformed value $\theta = \textrm{expit}(\alpha) = 1 / (1 + \exp(-\alpha))$:

```{r eval = inla_available, fig = TRUE}
plot(marg.theta, type = "l", main = "Posterior density",
  xlab = expression(theta), ylab = "density")
```


## Poisson model


Next, the analysis of the *Game of Thrones* data set will be considered and 
a Poisson model will be fitted:

```{r eval = inla_available}
inla.pois <- inla(Us ~ 1, data = list(Us = Us), 
  family = "poisson")
summary(inla.pois)
```

Again, in order to obtain the posterior marginal distribution of the mean
$\theta$,
the marginal of $\alpha$ needs to be transformed. In this case it is a bit
simpler since $\theta = \exp(\alpha)$:


```{r eval = inla_available}
marg.theta <- inla.tmarginal (exp, inla.pois$marginals.fixed[[1]]) 
```


```{r eval = inla_available, fig = TRUE}
plot(marg.theta, type = "l", main = "Posterior density",
  xlab = expression(theta), ylab = "density")
```

## Normal model

Finally, the heights data set will be considered to fit a Bayesian model
using a Gaussian likelihood with `INLA`:

```{r eval = inla_available}
inla.norm <- inla(heights ~ 1, data = list(heights = heights),
  family = "gaussian")
summary(inla.norm)
```

As the mean $\mu$ is equal to the intercept $\alpha$, there is no need
to make any transformation. Also, `INLA` provides an approximation
to the model precision, so both marginals can be plotted straight away:

```{r eval = inla_available, fig = TRUE}
par(mfrow = c(1, 2))
plot(inla.norm$marginals.fixed[[1]], type = "l", main = "Posterior density",
  xlab = expression(mu), ylab = "density")
plot(inla.norm$marginals.hyperpar[[1]], type = "l", main = "Posterior density",
  xlim = c(0, 400), xlab = expression(tau), ylab = "density")
```


