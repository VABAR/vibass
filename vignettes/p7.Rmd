---
title: 'Practical 7: Bayesian Hierarchical Modelling'
author: "VIBASS"
output:
  html_vignette:
    fig_caption: yes
    number_sections: yes
    toc: yes
    fig_width: 6
    fig_height: 4
vignette: >
  %\VignetteIndexEntry{Practical 7: Bayesian Hierarchical Modelling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Bayesian Hierarchical Modelling

In this last practical we will consider the analysis of Bayesian hierarchical
models. As explained in the previous lecture, hierarchical models provide a
convenient tool to define models so that the different sources of variation in
the data are clearly identified. Bayesian inference for highly structured
hierarchical models can be difficult and may require the use of Markov chain
Monte Carlo methods. However, packages such as `BayesX` and `INLA`, to mention
just two, provide a very convenient way to fit and make inference about complex
Bayesian hierarchical models.

Regarding software, we will use `MCMCpack` to fit the models in the fairly
straightforward examples in this Practical - more complex models would require
another package.

# Linear Mixed Models

Linear mixed models were defined in the lecture as follows:

$$
Y_{ij} = X_{ij}\beta +\phi_i+\epsilon_{ij}
$$

Here, Y_{ij} represents observation $j$ in group $i$, X_{ij} are a vector of
covariates with coefficients $\beta$, $\phi_i$ i.i.d. random effects and
$\epsilon_{ij}$ a Gaussian error term. The distribution of the random effects
$\phi_i$ is Gaussian with zero mean and precision $\tau_{\phi}$.


# Multilevel Modelling

Multilevel models are a particular type of mixed-effects models in which
observations are nested within groups, so that group effects are modelled using
random effects. A typical example is that of students nested within classes.

For the next example, the `nlschools` data set (in package `MASS`) will be used.
This data set records data about students' performance (in particular, about a
language score test) and other variables. The variables in this data set are:

* `lang`, language score test.

* `IQ`, verbal IQ.

* `class`, class ID.

* `GS`, class size as number of eighth-grade pupils recorded in the class.

* `SES`, social-economic status of pupilâ€™s family.

* `COMB`, whether the pupils are taught in the multi-grade class with 7th-grade students.

The data set can be loaded and summarised as follows:

```{r}
library("MASS")
data("nlschools")
summary(nlschools)
```

The model to fit will take `lang` as the response variable and include
`IQ`, `GS`, `SES` and `COMB` as covariates (i.e., fixed effects). This model
can easily be fitted with `MCMCregress` as follows:

```{r message = FALSE, warning = FALSE}
library(MCMCpack)
burnin <- 5000
mcmc <- 10000
thin <- 10
results1 <- MCMCregress(formula = lang ~ IQ + GS +  SES + COMB,
                  data = nlschools,
                  b0 = 0, B0 = 0.0001,
                  c0 = 2, d0 = 2, # Because the prior is Ga(c0/2,d0/2),
                  beta.start = c(1,1),
                  burnin=burnin, mcmc=mcmc, thin=thin,
                  verbose=1000)
summary(results1)
```

Note that the previous model only includes fixed effects. The data set includes
`class` as the class ID to which each student belongs. Class effects can have an
impact on the performance of the students, with students in the same class
performing similarly in the language test.

Very conveniently, the function `MCMChregress` in `MCMCpack"
can include random effects in the model. The argument `group` takes the name
of the grouping variable (in quotes). The fixed effects are written as a 
standard model formula using the argument `fixed`, while the argument `random`
(as you might expect!) contains the random effects model formula, this time
using a one-sided formula (as the response variable was already declared
in the fixed model formula). This is best illustrated via example, as below.

Before fitting the model, the between-class variability can be explored by
means of boxplots:

```{r fig = TRUE, fig.width = 15, fig.height = 5}
boxplot(lang ~ class, data = nlschools, las = 2)
```

The we wish to use has a random effect indexed over grouping variable
`class` and random effects which are independent and identically distributed
using a normal distribution with zero mean and precision $\tau_\phi$.
The function `MCMChregress()` places a Wishart prior on $\tau_\phi$,
but studying this is beyond the scope of this course. For our purposes,
we would like a vague prior for this quantity (similar to the Uniform prior
placed on $\sigma_\phi$ in the lectures), and this equates to placing an
Inverse Wishart prior on $\sigma^2_\phi$. We use a IW(1,1) prior here, to
give the most uninformative prior (via arguments $r$ and $R$ to
`MCMChregress()`).

The normal priors for the fixed effect regression parameters have means set
by `mubeta` and variances by `Vbeta`; we use fixed values of 0 for each mean
and 0.000001 for each variance.

Note that in `MCMChregress()`, the argument `verbose` only takes the values `0`
and `1`, representing a switch where a progress bar is shown if the value `1`
is chosen.

The code to fit the model with a single random effect allowing for a different
mean level per group is:

```{r}
results2 <- MCMChregress(
  fixed = lang ~ IQ + GS +  SES + COMB,
  random = ~ 1,
  group = "class",
  data = nlschools,
  #burnin=1000, mcmc=10000, thin=1, verbose=1,
  burnin=100, mcmc=1000, thin=1, verbose=1,
  #beta.start=0, sigma2.start=1, # Starting values for 
  mubeta=0, Vbeta=1.0E6, # Mean and variance of prior distributions for each random effect
  r=1, R=1 # To give an uninformative prior on sigma_phi
)

```

If we look at
```{r}
summary(results2)
```
all we see is a brief description of the stucture of the MCMC output contained
within `results2`. This means that to look at the results, we need to do a 
little more work.

There are two objects in `results2`; `mcmc` contains the MCMC simulations, and
`Y.pred` contains predictions for the response variable, `lang` in this
example.

Doing
```{r}
summary(results2$mcmc)
```
gives summary statistics for *everything* in the model - including all of the
random effects. Most often we want to study the fixed effects; we can select
*only* those terms using the somewhat clunky but powerful code below,.
making use of `grepl`, a function for matching regular expressions:
```{r}
summary(results2$mcmc[ , grepl("beta", colnames(results2$mcmc), fixed=TRUE)])
```
We are also interested in the variance terms for the random effects and for
the residuals - the relative sizes of these in informative on whether there is
more variability within the groups or between the groups.

For the residual variance, we use
```{r}
summary(results2$mcmc[ , grepl("sigma2", colnames(results2$mcmc), fixed=TRUE)])
```
and for the random effects variance, we use
```{r}
summary(results2$mcmc[ , grepl("VCOV", colnames(results2$mcmc), fixed=TRUE)])
```
The random effect variance term looks more complicated because for more
complex models (with more than a single random effect term) it will be a matrix.

In this example, we can see that the random effect variance is much larger than
the residual variance, suggesting that there is more variability between the
groups than within the groups.

Finally, we can study some trace plots and some density plots.

```{r fig = TRUE, fig.width = 7, fig.height = 9}
    par(mfrow=c(3,2))
    ts.plot(results2$mcmc[,"beta.(Intercept)"], ylab = "beta.Intercept", xlab = "Iteration")
    ts.plot(results2$mcmc[,"beta.IQ"], ylab = "beta.IQ", xlab = "Iteration")
    ts.plot(results2$mcmc[,"beta.GS"], ylab = "beta.GS", xlab = "Iteration")
    ts.plot(results2$mcmc[,"beta.SES"], ylab = "beta.SES", xlab = "Iteration")
    ts.plot(results2$mcmc[,"beta.COMB1"], ylab = "beta.COMB1", xlab = "Iteration")
```

```{r fig = TRUE, fig.width = 7, fig.height = 9}
    par(mfrow=c(3,2))
    ts.plot(results2$mcmc[,"VCV.(Intercept).(Intercept)"], ylab = "RE Variance", xlab = "Iteration")
    ts.plot(results2$mcmc[,"sigma2"], ylab = "Residual Variance", xlab = "Iteration")
```

```{r fig = TRUE, fig.width = 7, fig.height = 9}
    par(mfrow=c(3,2))
    plot(density(results2$mcmc[,"beta.(Intercept)"], bw = 5),
         main = "Posterior density for beta.Intercept")
    plot(density(results2$mcmc[,"beta.IQ"], bw = 0.2),
         main = "Posterior density for beta.IQ")
    plot(density(results2$mcmc[,"beta.GS"], bw = 0.75),
         main = "Posterior density for beta.GS")
    plot(density(results2$mcmc[,"beta.SES"], bw = 0.2),
         main = "Posterior density for beta.SES")
    plot(density(results2$mcmc[,"beta.COMB1"], bw = 5),
         main = "Posterior density for beta.COMB1")
```

```{r fig = TRUE, fig.width = 7, fig.height = 9}
    par(mfrow=c(3,2))
    plot(density(results2$mcmc[,"VCV.(Intercept).(Intercept)"], bw = 0.5),
         main = "Posterior density for RE Variance")
    plot(density(results2$mcmc[,"sigma2"], bw = 0.5),
         main = "Posterior density for Residual Variance")
```

# Generalised Linear Mixed Models

Mixed effects models can also be considered within the context of generalised 
linear models. In this case, the linear predictor of observation $i$, $\eta_i$,
can be defined as

$$
\eta_i = X_{ij}\beta +\phi_i
$$

Compared to the previous setting of linear mixed effects models, note that now
the distribution of the response could be other than Gaussian and that
observations are not necessarily nested within groups.

# Poisson regression

In this practical we will use the North Carolina Sudden Infant Death Syndrome
(SIDS) data set. It is available in the `spData` package and it can be loaded
using:

```{r message = FALSE, warning = FALSE}
library(spData)
data(nc.sids)
summary(nc.sids)
```

A full description of the data set is provided in the associated manual page
(check with `?nc.sids`) but in this practical we will only consider these
variables:

* `BIR74`,  number of births (1974-78).

* `SID74`,  number of SID deaths (1974-78).

* `NWBIR74`, number of non-white births (1974-78).

These variables are measured at the county level in North Carolina, of which
there are 100.

Because `SID74` records the number of SID deaths, the model is Poisson:

$$
O_i \mid \mu_i \sim Po(\mu_i),\ i=1,\ldots, 100
$$
Here, $O_i$ represents the number of cases in county $i$ and $\mu_i$ the mean.
In addition, mean $\mu_i$ will be written as $\mu_i = E_i \theta_i$, where $E_i$
is the *expected* number of cases and $\theta_i$ the relative risk in county $i$.

The relative risk $\theta_i$ is often modelled, on the log-scale, to be equal
to a linear predictor:

$$
\log(\theta_i) = \beta_0 + \ldots
$$

The expected number of cases is computed by multiplying the number of births in
county $i$ to the overall mortality rate

$$
r = \frac{\sum_{i=1}^{100}O_i}{\sum_{i=1}^{100}B_i}
$$
where $B_i$ represents the total number of births in country $i$. Hence,
the expected number of cases in county $i$ is $E_i = r B_i$.


```{r}
# Overall mortality rate
r74 <- sum(nc.sids$SID74) / sum(nc.sids$BIR74)
# Expected cases
nc.sids$EXP74 <- r74 * nc.sids$BIR74
```

A common measure of relative risk is the *standardised mortality ratio*
($O_i / E_i$):

```{r}
nc.sids$SMR74 <- nc.sids$SID74 / nc.sids$EXP74
```

A summary of the SMR can be obtained:

```{r fig = TRUE}
hist(nc.sids$SMR, xlab = "SMR")
```

Values above 1 indicate that the county has more observed deaths than expected
and that there might be an increased risk in the area.

As a covariate, we will compute the proportion of non-white births:

```{r}
nc.sids$NWPROP74 <- nc.sids$NWBIR74/ nc.sids$BIR74
```

There is a clear relationship between the SMR and the proportion of non-white
births in a county:

```{r fig = TRUE}
plot(nc.sids$NWPROP74, nc.sids$SMR74)

# Correlation
cor(nc.sids$NWPROP74, nc.sids$SMR74)
```


A simple Poisson regression can be fitted by using the following code:

```{r}
library(INLA)
m1nc <- inla(
  SID74 ~ 1 + NWPROP74,
  family = "poisson",
  E = nc.sids$EXP74,
  data = nc.sids
)
summary(m1nc)
```

Random effects can also be included to account for intrinsic differences between
the counties:

```{r}
# Index for random effects
nc.sids$ID <- 1:nrow(nc.sids)

# Model WITH covariate
m2nc <- inla(
  SID74 ~  1 + NWPROP74 + f(ID, model = "iid"),
  family = "poisson",
  E = nc.sids$EXP74,
  data = as.data.frame(nc.sids)
)

summary(m2nc)
```

The role of the covariate can be explored by fitting a model without it:


```{r}
# Model WITHOUT covariate
m3nc <- inla(
  SID74 ~  1 + f(ID, model = "iid"),
  family = "poisson",
  E = nc.sids$EXP74,
  data = as.data.frame(nc.sids)
)

summary(m3nc)
```

Now, notice the decrease in the estimate of the precision of the random effects
(i.e., the variance increases). This means that values of the random effects
are now larger than in the previous case as the random effects pick some of the
effect explained by the covariate.


```{r fig = TRUE}
par(mfrow = c(1, 2))
boxplot(m2nc$summary.random$ID$mean, ylim = c(-1, 1), main = "With NWPROP74")
boxplot(m3nc$summary.random$ID$mean, ylim = c(-1, 1), main = "Without NWPROP74")
```

# Further Extensions

Spatial random effects can be defined not to be independent and identically
distributed. Instead, spatial or temporal correlation can be considered when
defining them. For example, in the North Carolina SIDS data set, it is common
to consider that two counties that are neighbours (i.e., share a boundary)
will have similar relative risks. This can be taken into account in the model
but assuming that the random effects are spatially autocorrelated. This is out
of the scope of this introductory course but feel free to ask about this!!
