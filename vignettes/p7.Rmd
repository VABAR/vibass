---
title: 'Practical 7: Generalized linear models'
author: "VIBASS"
date: "July 2022"
output:
  html_vignette:
    fig_caption: yes
    number_sections: yes
    toc: yes
    fig_width: 6
    fig_height: 4
vignette: >
  %\VignetteIndexEntry{Practical 7: Generalized linear models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


# Introduction

In the previous practical, Bayesian inference for linear models using Importance
Sampling (IS) and the Metropolis-Hastings (M-H) algorithms has been introduced.
In this practical you will be working with generalized linear models (GLMs)
on the binomial and Poisson examples described earlier.
In particular, you will be implementing the IS and M-H algorithms for
Bayesian inference on GLMs. In case you need to refresh yourself on the details about
IS and M-H, please check the lecture notes or Practical 5.


# Example: Binomial-Beta Model

This data set is described in detail in previous practicals; it concerns
the recorded the number of red M&M's in tube $i$ with $n_i$ M&M's for
different tubes.

The model can be stated as follows:

$$
\begin{array}{rcl}
y_i \mid \theta & \sim & Bi(n_i, \theta)\\
\theta & \sim & Be(1, 1)
\end{array}
$$

You can use the following data set for this exercise:

```{r eval = TRUE}
data <- data.frame(MMs = c(20, 22, 24), red = c(5, 8, 9))
```

These data reproduce different counts of red M&M's in three different
tubes, with `MMs`recording the total number of M&M's in the tube
and `red` the number of red ones.

Please check Practical 5 in case you need to remember the details of the
implementation of the IS and M-H algorithms for this particular model.  Also,
you can take the `R` code from Practical 5 and modify it to complete the
exercise above.

## Exercises


### Generalized linear models


The current model can be expressed as a generalized linear model as follows:

$$
\begin{array}{rcl}
y_i & \sim & Bi(\theta, n_i)\\
\textrm{logit}(\theta) & = & \beta_0\\
\beta_0 & \sim & N(0, \tau_{\beta_0} = 0.001)
\end{array}
$$


Note that now the model parameter $\theta$ is linked to a
linear predictor ($\beta_0$ in this case) via the logit link function.
Hence, the model parameter is now $\beta_0$.

This GLM can be easily fitted using IS and M-H by treating $\beta_0$
as the unique model parameter.

### Importance sampling

In order to fit this model using the IS algorithm, values of $\beta_0$
need to be sampled. Given that $\beta_0$ is not bounded, we can use
a Normal distribution with zero mean and precision 0.1. Alternatively,
the mean of the sampling distribution can be set to a more reasonable
value, close to the logit of the average proportions of red
M&Ms in the sample (for example, -1.5).

* Implement the IS algorithm using the two sampling distributions proposed above, i.e., a Normal with zero mean and precision 0.1 and a Normal with mean -1.5 and precision 0.1.

* Compute the posterior mean and variance for both sets of results.
Are they similar?

* Compute and compare the effective sample sizes obtained with both
sampling distributions. What do you find? Why do you think that this happens?



### Metropolis-Hastings

Similarly, the previous GLM can be fit using the M-H algorithm. Now, values
of $\beta_0$ are proposed, for which a Normal distribution centered at
the current value and precision 0.1 can be used. However, the precision
value can be tuned if proposed values are rejected too often or not
rejected at all.


* Implement the M-H algorithm for this example.

* Compute the posterior mean and variance of $\beta_0$ and $\theta$.

* Compare these results to those obtained with the IS algorithm.


# Example: Poisson-Gamma Model

The second example will be based on the *Game of Thrones* data set, which
has also been described in previous practicals.
Remember that this is made of the observed number of u's on
a page of a book of Game of Thrones. The model can be stated as:

$$
\begin{array}{rcl}
y_i & \sim & Po(\theta)\\
\theta & \sim & Ga(0.01, 0.01)
\end{array}
$$

We will denote the observed values by `y` in the `R` code. The data can be
loaded with:

```{r eval = TRUE}
data <- data.frame(Us = c(25, 29, 27, 27, 25, 27, 22, 26, 27, 29, 23, 28, 25,
  24, 22, 25, 23, 29, 23, 28, 21, 29, 28, 23, 28))
y <- data$Us
```

Again, please check Practical 5 if you need to recall the details of
the implementation of the IS and M-H algorithms for this particular
example. You can also use the `R` code from Practical 5 to develop the following
exercises.

## Exercises

### Generalized linear models


The current model can be expressed as a generalized linear model as follows:

$$
\begin{array}{rcl}
y_i & \sim & Po(\theta)\\
\log(\theta) & = & \beta_0\\
\beta_0 & \sim & N(0, \tau_{\beta_0} = 0.001)
\end{array}
$$


Note that now the model parameter $\theta$ is linked to a
linear predictor ($\beta_0$ in this case) via the logarithm link function.
Hence, the model parameter is now $\beta_0$.

This GLM can be easily fitted using IS and M-H by treating $\beta_0$
as the unique model parameter.

### Importance sampling

In order to fit this model using the IS algorithm, values of $\beta_0$
need to be sampled. Given that $\beta_0$ is not bounded, we can use
a Normal distribution with zero mean and precision 0.1. Alternatively,
the mean of the sampling distribution can be set to a more reasonable
value, close to the logarithm of the average number of u's 
in the sample (for example, 3.25).

* Implement the IS using the two sampling distributions proposed above, i.e.,
a Normal with zero mean and precision 0.1 and a Normal with mean 3.25 and 
precision 0.1.

* Compute the posterior mean and variance for both sets of results.

* Compute and compare the effective sample sizes obtained with both
sampling distributions. What do you find? Why do you think that this happens?



### Metropolis-Hastings

Again, the previous GLM can be fitted using the M-H algorithm. Now, values
of $\beta_0$ are proposed, for which a Normal distribution centered at
the current value and precision 0.1 can be used. However, the precision
value can be tuned if proposed values are rejected too often or not
rejected at all.


* Implement the M-H algorithm for this model.

* Compute the posterior mean and variance of $\beta_0$ and $\theta$.

* Compare the results obtained now with those obtained with the I-S algorithm.

