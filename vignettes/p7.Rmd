---
title: 'Practical 7: Bayesian Hierarchical Modelling'
author: "VIBASS"
output:
  html_vignette:
    fig_caption: yes
    number_sections: yes
    toc: yes
    fig_width: 6
    fig_height: 4
vignette: >
  %\VignetteIndexEntry{Practical 7: Bayesian Hierarchical Modelling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Bayesian Hierarchical Modelling

In this last practical we will consider the analysis of Bayesian hierarchical
models. As explained in the previous lecture, hierarchical models provide a
convenient tool to define models so that the different sources of variation in
the data are clearly identified. Bayesian inference for highly structured
hierarchical models can be difficult and may require the use of Markov chain
Monte Carlo methods. However, packages such as `BayesX` and `INLA`, to mention
just two, provide a very convenient way to fit and make inference about complex
Bayesian hierarchical models.

Regarding software, we will use `MCMCpack` to fit the models in the fairly
straightforward examples in this Practical - more complex models would require
another package.

# Linear Mixed Models

Linear mixed models were defined in the lecture as follows:

$$
Y_{ij} = X_{ij}\beta +\phi_i+\epsilon_{ij}
$$

Here, Y_{ij} represents observation $j$ in group $i$, X_{ij} are a vector of
covariates with coefficients $\beta$, $\phi_i$ i.i.d. random effects and
$\epsilon_{ij}$ a Gaussian error term. The distribution of the random effects
$\phi_i$ is Gaussian with zero mean and precision $\tau_{\phi}$.


# Multilevel Modelling

Multilevel models are a particular type of mixed-effects models in which
observations are nested within groups, so that group effects are modelled using
random effects. A typical example is that of students nested within classes.

For the next example, the `nlschools` data set (in package `MASS`) will be used.
This data set records data about students' performance (in particular, about a
language score test) and other variables. The variables in this data set are:

* `lang`, language score test.

* `IQ`, verbal IQ.

* `class`, class ID.

* `GS`, class size as number of eighth-grade pupils recorded in the class.

* `SES`, social-economic status of pupilâ€™s family.

* `COMB`, whether the pupils are taught in the multi-grade class with 7th-grade students.

The data set can be loaded and summarised as follows:

```{r}
library("MASS")
data("nlschools")
summary(nlschools)
```

The model to fit will take `lang` as the response variable and include
`IQ`, `GS`, `SES` and `COMB` as covariates (i.e., fixed effects). This model
can easily be fitted with `MCMCregress` as follows:

```{r message = FALSE, warning = FALSE}
library(MCMCpack)
burnin <- 5000
mcmc <- 10000
thin <- 10
results1 <- MCMCregress(formula = lang ~ IQ + GS +  SES + COMB,
                  data = nlschools,
                  b0 = 0, B0 = 0.0001,
                  c0 = 2, d0 = 2, # Because the prior is Ga(c0/2,d0/2),
                  beta.start = c(1,1),
                  burnin=burnin, mcmc=mcmc, thin=thin,
                  verbose=1000)
summary(results1)
```

Note that the previous model only includes fixed effects. The data set includes
`class` as the class ID to which each student belongs. Class effects can have an
impact on the performance of the students, with students in the same class
performing similarly in the language test.

Very conveniently, the function `MCMChregress` in `MCMCpack"
can include random effects in the model. The argument `group` takes the name
of the grouping variable (in quotes). The fixed effects are written as a 
standard model formula using the argument `fixed`, while the argument `random`
(as you might expect!) contains the random effects model formula, this time
using a one-sided formula (as the response variable was already declared
in the fixed model formula). This is best illustrated via example, as below.

Before fitting the model, the between-class variability can be explored by
means of boxplots:

```{r fig = TRUE, fig.width = 15, fig.height = 5}
boxplot(lang ~ class, data = nlschools, las = 2)
```

The we wish to use has a random effect indexed over grouping variable
`class` and random effects which are independent and identically distributed
using a normal distribution with zero mean and precision $\tau_\phi$.
The function `MCMChregress()` places a Wishart prior on $\tau_\phi$,
but studying this is beyond the scope of this course. For our purposes,
we would like a vague prior for this quantity (similar to the Uniform prior
placed on $\sigma_\phi$ in the lectures), and this equates to placing an
Inverse Wishart prior on $\sigma^2_\phi$. We use a IW(1,1) prior here, to
give the most uninformative prior (via arguments $r$ and $R$ to
`MCMChregress()`).

The normal priors for the fixed effect regression parameters have means set
by `mubeta` and variances by `Vbeta`; we use fixed values of 0 for each mean
and 0.000001 for each variance.

Note that in `MCMChregress()`, the argument `verbose` only takes the values `0`
and `1`, representing a switch where a progress bar is shown if the value `1`
is chosen.

The code to fit the model with a single random effect allowing for a different
mean level per group is:

```{r}
results2 <- MCMChregress(
  fixed = lang ~ IQ + GS +  SES + COMB,
  random = ~ 1,
  group = "class",
  data = nlschools,
  #burnin=1000, mcmc=10000, thin=1, verbose=1,
  burnin=100, mcmc=1000, thin=1, verbose=1,
  #beta.start=0, sigma2.start=1, # Starting values for 
  mubeta=0, Vbeta=1.0E6, # Mean and variance of prior distributions for each random effect
  r=1, R=1 # To give an uninformative prior on sigma_phi
)

```

If we look at
```{r}
summary(results2)
```
all we see is a brief description of the stucture of the MCMC output contained
within `results2`. This means that to look at the results, we need to do a 
little more work.

There are two objects in `results2`; `mcmc` contains the MCMC simulations, and
`Y.pred` contains predictions for the response variable, `lang` in this
example.

Doing
```{r}
summary(results2$mcmc)
```
gives summary statistics for *everything* in the model - including all of the
random effects. Most often we want to study the fixed effects; we can select
*only* those terms using the somewhat clunky but powerful code below,.
making use of `grepl`, a function for matching regular expressions:
```{r}
summary(results2$mcmc[ , grepl("beta", colnames(results2$mcmc), fixed=TRUE)])
```
This works because all the names of the fixed effects (including the overall
Intercept) contain the string `beta`.

We are also interested in the variance terms for the random effects and for
the residuals - the relative sizes of these in informative on whether there is
more variability within the groups or between the groups.

For the residual variance, we use
```{r}
summary(results2$mcmc[ , grepl("sigma2", colnames(results2$mcmc), fixed=TRUE)])
```
and for the random effects variance, we use
```{r}
summary(results2$mcmc[ , grepl("VCOV", colnames(results2$mcmc), fixed=TRUE)])
```
The random effect variance term looks more complicated because for more
complex models (with more than a single random effect term) it will be a matrix.

In this example, we can see that the random effect variance is much larger than
the residual variance, suggesting that there is more variability between the
groups than within the groups.

Finally, we can study some trace plots and some density plots.

```{r fig = TRUE, fig.width = 7, fig.height = 9}
    par(mfrow=c(3,2))
    ts.plot(results2$mcmc[,"beta.(Intercept)"], ylab = "beta.Intercept", xlab = "Iteration")
    ts.plot(results2$mcmc[,"beta.IQ"], ylab = "beta.IQ", xlab = "Iteration")
    ts.plot(results2$mcmc[,"beta.GS"], ylab = "beta.GS", xlab = "Iteration")
    ts.plot(results2$mcmc[,"beta.SES"], ylab = "beta.SES", xlab = "Iteration")
    ts.plot(results2$mcmc[,"beta.COMB1"], ylab = "beta.COMB1", xlab = "Iteration")
```

```{r fig = TRUE, fig.width = 7, fig.height = 9}
    par(mfrow=c(3,2))
    ts.plot(results2$mcmc[,"VCV.(Intercept).(Intercept)"], ylab = "RE Variance", xlab = "Iteration")
    ts.plot(results2$mcmc[,"sigma2"], ylab = "Residual Variance", xlab = "Iteration")
```

```{r fig = TRUE, fig.width = 7, fig.height = 9}
    par(mfrow=c(3,2))
    plot(density(results2$mcmc[,"beta.(Intercept)"], bw = 5),
         main = "Posterior density for beta.Intercept")
    plot(density(results2$mcmc[,"beta.IQ"], bw = 0.2),
         main = "Posterior density for beta.IQ")
    plot(density(results2$mcmc[,"beta.GS"], bw = 0.75),
         main = "Posterior density for beta.GS")
    plot(density(results2$mcmc[,"beta.SES"], bw = 0.2),
         main = "Posterior density for beta.SES")
    plot(density(results2$mcmc[,"beta.COMB1"], bw = 5),
         main = "Posterior density for beta.COMB1")
```

```{r fig = TRUE, fig.width = 7, fig.height = 9}
    par(mfrow=c(3,2))
    plot(density(results2$mcmc[,"VCV.(Intercept).(Intercept)"], bw = 0.5),
         main = "Posterior density for RE Variance")
    plot(density(results2$mcmc[,"sigma2"], bw = 0.5),
         main = "Posterior density for Residual Variance")
```

# Generalised Linear Mixed Models

Mixed effects models can also be considered within the context of generalised 
linear models. In this case, the linear predictor of observation $i$, $\eta_i$,
can be defined as

$$
\eta_i = X_{ij}\beta +\phi_i
$$

Compared to the previous setting of linear mixed effects models, note that now
the distribution of the response could be other than Gaussian and that
observations are not necessarily nested within groups.

# Poisson GLMM - overdispersion 

In the previous practical we looked at the Salmonella example with a Poisson
GLM, and we saw evidence that a standard GLM was not adequate to describe the
amount of variability in the data set, in terms of the number of colonies.

In this practical we will fit a Poisson GLMM as a mechanism for introducing
extra variation into the model; another way of thinking of this is that we
will be including a term in the model to deal with *overdispersion*.

If you need to load the data set in again:
```{r}
salmonella <- read.csv("..\\data\\salmonella.csv")
```

If not loaded already, the package must be loaded into R:
```{r echo=FALSE}
library(MCMCpack)
```

If not loaded already, the package must be loaded into R:
```{r echo=FALSE}
library(R2BayesX)
```

A Poisson GLMM to deal with overdispersion simply has a separate random effect
term $\lambda_i$ for each data point; hence our model becomes:
$ y_i = \beta_0 + \beta_1 \log \left(x_i+10\right) + \beta_2x_i + \lambda_i$
where
$\lambda_i\sim\mbox{N}\left(0,\sigma^2)$.

`MCMCpack` places an Inverse Wishart prior distribution on the random effect
distribution variance term; we need to specify the variance matrix, which in
our case will be a 1 by 1 matrix.

Now the model fitting:
```{r fig=TRUE}
# Index for random effects
salmonella$ID <- 1:nrow(salmonella)
salmonella$ID
ID <- salmonella$ID

# Fit the model
salmonella$logDose10 <- log(salmonella$Dose+10)
results3 <- bayesx(y ~ logDose10 + Dose + sx(ID, bs = "re"),
                   family="poisson", data = salmonella)
summary(results3)
plot(results3)

# Fit the model
# salmonella$log.Dose10 <- log(salmonella$Dose+10)
# results3 <- MCMChpoisson(fixed = y ~ log.Dose10 + Dose,
#                          random = ~ 1,
#                          group = "ID",
#                          data = salmonella,
#                          burnin = 5000, mcmc = 10000, thin=10,
#                          # beta.start = c(NA,NA,NA), # uses maximum likelihood estimates as starting values when NA
#                          mubeta = c(0,1,1),
#                          Vbeta = 0.0001,
#                          R = diag(1,1),
#                          FixOD=1, verbose=1)
# # Summarise output
# summary(results3)

```

## Exercises

* Use BayesX

```{r fig = TRUE, echo=TRUE}
    plot(salmonella$Dose,salmonella$y)#,ylab="Number of colonies")
    x.predict <- seq(0,1000,length=1001)
    beta.0 <- coef(results3)["(Intercept)","Mean"]
    beta.0
    beta.1 <- coef(results3)["logDose10","Mean"]
    beta.2 <- coef(results3)["Dose","Mean"]
    y.predict <- exp(beta.0 + beta.1 * log(x.predict+10) + beta.2 * x.predict)
    lines(x.predict,y.predict)
    # Now let's plot the prediction intervals - here we need to use the full set of MCMC samples, and add on a contribution for the random effect variance
    beta.samples <- samples(results3) # Extract MCMC samples for the betas
    RE.sd <- sqrt(mean(samples(results3, term = "sx(ID):re")[,"Var"])) # RE standard deviation
    x.pred.matrix <- as.matrix(x.predict) # Needed to use apply()
    y.pred.matrix <- apply(x.pred.matrix,1,function(x){beta.samples[,"Intercept"]+beta.samples[,"logDose10"]*log(x+10)+beta.samples[,"Dose"]*x})
    y.predict.lower <- apply(y.pred.matrix,2,function(x){quantile(x,0.025)})
    y.predict.lower <- exp(y.predict.lower)#-1.96*RE.sd)
    lines(x.predict,y.predict.lower,lty=2)
    y.predict.upper <- apply(y.pred.matrix,2,function(x){quantile(x,0.975)})
    y.predict.upper <- exp(y.predict.upper)#+1.96*RE.sd)
    lines(x.predict,y.predict.upper,lty=2)
```


# Further Extensions

Spatial random effects can be defined not to be independent and identically
distributed. Instead, spatial or temporal correlation can be considered when
defining them. With spatial area data sets for example, it is common
to consider that two areas that are neighbours (i.e., share a boundary)
will have some correlation between them, This can be taken into account in
GLMMs,
but assuming that the random effects are spatially autocorrelated. This is out
of the scope of this introductory course but feel free to ask about this!!
