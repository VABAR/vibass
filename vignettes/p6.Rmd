---
title: 'Practical 6: Software and GLMs'
author: "VIBASS"
output:
  html_vignette:
    fig_caption: yes
    number_sections: yes
    toc: yes
    fig_width: 6
    fig_height: 4
vignette: >
  %\VignetteIndexEntry{Practical 6: Software and GLMs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  collapse = TRUE,
  comment = "#>"
)
```

# Software for Bayesian Statistical Analysis

So far, simple Bayesian models with conjugate priors have been
considered.  As explained in previous practicals, when the posterior
distribution is not available in closed form, MCMC algorithms such as the
Metropolis-Hastings or
Gibbs Sampling can be used to obtain samples from it.

In general, posterior distributions are seldom available in closed form and
implementing MCMC algorithms for complex models can be technically difficult
and very time-consuming.

For this reason, in this Practical we start by listing a number of `R`
packages to fit Bayesian statistical models. These packages equip researchers
with tools for dealing with complex models efficiently,
without having to do a lot of extra mathematics and coding. Fitting Bayesian
models in `R` is then much like fitting non-Bayesian models, using
model-fitting functions at the command line, and using standard syntax for
model specification.

## Specific Software

* We have already seen package `MCMCpack` in R, which contains functions
such as `MCMClogit()`, `MCMCPoisson()` and `MCMCprobit()` for fitting
specific kinds of models via simple function calls.
* `BayesX` (http://www.bayesx.org/) implements MCMC methods to obtain samples
from the joint posterior and is conveniently accessed from R via the package
`R2BayesX`. It has a very simple interface to define models.
using a `formula` (in the same way as with `glm()` and `gam()` functions).
* `INLA` (https://www.r-inla.org/) is based on producing (accurate)
approximations to the marginal posterior distributions of the model parameters.
Although this can be enough most of the time, making multivariate inference
with `INLA` can be difficult or impossible. However, in many cases this is
not needed and `INLA` can fit some classes of models in a fraction of the
time it takes with MCMC.  It has a very simple interface to define models,
although it cannot be installed directly from CRAN - instead you have a
specific website where it can be downloaded:
[https://www.r-inla.org/download-install](https://www.r-inla.org/download-install)
* A classic MCMC program is `BUGS`, (Bayesian Analysis using Gibbs Sampling)
described in Lunn et al. (2000):
[http://www.mrc-bsu.cam.ac.uk/bugs/winbugs/contents.shtml](http://www.mrc-bsu.cam.ac.uk/bugs/winbugs/contents.shtml).
`BUGS` can be used through graphical interfaces `WinBUGS` and `OpenBUGS`. Both of
these packages can be called from within R using packages `R2WinBUGS` and
`R2OpenBUGS`. There also exists software called
[`multibugs`](https://www.multibugs.org/) which is a parallel implementation of
BUGS.
* `JAGS`, which stands for “just another Gibbs sampler”. Can also be called from
R using package `r2jags`.
* The `NIMBLE` package extends `BUGS` and implements MCMC and other methods
for Bayesian inference. You can get it from
[https://r-nimble.org](https://r-nimble.org), and is best
run directly from R.
* The `Stan` software implements Hamiltonian Monte Carlo and other methods for
fitting hierarchical Bayesian models. It is available from
[https://mc-stan.org](https://mc-stan.org).

# Bayesian Logistic Regression

## Model Formulation

To summarise the model formulation presented in the lecture, given a response
variable $Y_i$ representing the count of a number of successes from a given
number of trials $n_i$ with success probability $\theta_i$, we have

* $(Y_i \mid \boldsymbol \theta_i) \sim\mbox{Bi}(n_i, \theta_i),\,\, i.i.d.\,\,, i=1, \ldots, m$
\begin{align*}
 \mbox{logit}(\theta_i) & =\eta_i \nonumber\\
\eta_{i} & =\beta_0+\beta_1 x_{i1}+\ldots+\beta_p x_{ip}=\boldsymbol x_i\boldsymbol \beta\nonumber
\end{align*}
assuming the logit link function and with linear predictor $\eta_{i}$.

## Example: Fake News

The `fake_news` data set in the `bayesrules` package in `R` contains
information about 150 news articles, some real news and some fake news.

In this example, we will look at trying to predict whether an article of news
is fake or not given three explanatory variables.

We can use the following code to extract the variables we want from the
data set:

```{r}
library(bayesrules)
fakenews <- fake_news[,c("type","title_has_excl","title_words","negative")]
```

The response variable `type` takes values `fake` or `real`, which should be
self-explanatory. The three explanatory variables are:

* `title_has_excl`, whether or not the article contains an excalamation mark (values `TRUE` or `FALSE`);

* `title_words`, the number of words in the title (a positive integer); and

* `negative`, a sentiment rating, recorded on a continuous scale.

In the exercise to follow, we will examine whether the chance of an article
being fake news is related to the three covariates here.

## Fitting Bayesian Logistic Regression Models

We will analyse these data using the `MCMClogit()` function in package
`MCMCpack`. Note that this function uses a Metropolis-Hastings algorithm to
fit the model, rather than the Gibbs Sampler as used by `MCMCregress()`.

The package must first be loaded into R:
```{r}
library(MCMCpack)
```

The syntax for fitting a Bayesian Logistic Regression Model with one response
variable and three explanatory variables will be like so:

```{r eval=FALSE}
model1 <- MCMClogit(formula = y ~ x1 + x2 + x3,
                 data = data.set)
```
with extra options for burn-in (`burnin`), MCMC samples (`mcmc`), and thinning
(`thin`) as with `MCMCregress()`.

## Model Fitting

Note that the variable `title_has_excl` will need to be either replaced by or
converted to a factor, for example

```{r}
fakenews$titlehasexcl <- as.factor(fakenews$title_has_excl)
```

Function `summary` produces a summary (including parameter
estimates etc) for the parameters.

<!-- In order to be able to obtain output plots from BayesX, it seems that we need -->
<!-- to create a new version of the response variable of type logical: -->

```{r}
fakenews$typeFAKE <- fakenews$type == "fake"
```

## Exercises

* Perform an exploratory assessment of the fake news data set, in particular
looking at the possible relationships between the explanatory variables
and the fake/real response variable `typeFAKE`. You may wish to use the R 
function `boxplot()` here.

    <details><summary>Solution</summary>
    
    ```{r fig = TRUE}
    # Is there a link between the fakeness and whether the title has an exclamation mark?
    table(fakenews$title_has_excl, fakenews$type)
    # For the quantitative variables, look at boxplots on fake vs real
    boxplot(fakenews$title_words ~ fakenews$type)
    boxplot(fakenews$negative ~ fakenews$type)

    ```
    
    </details>
    
* Fit a Bayesian model in MCMCpack using the fake news `typeFAKE` variable as
response and the others as covariates. Examine the output; does the model fit
well, and is there any evidence that any of the explanatory variables are
associated with changes in probability of an article being fake or not?

    <details><summary>Solution</summary>
    
    ```{r fig = TRUE}
    # Fit the logistic regression model
    results1 <- MCMClogit(formula = typeFAKE ~ titlehasexcl + title_words + negative,
    data = fakenews,
    burnin = 5000, mcmc = 15000, thin=1,
    beta.start = NA, # uses maximum likelihood estimates as starting values
    b0 = c(0,0,0,0), B0 = c(0.0001,0.0001,0.0001,0.0001),
    verbose=1000)
    summary(results1)
    ```
    
    </details>

* Produce plots of the MCMC sample traces and the estimated posterior
distributions for the model parameters. Does it seem like convergence has been
achieved?

    <details><summary>Solution</summary>
    
    ```{r fig = TRUE, fig.width = 5, fig.height = 10}
    # Trace plots
    par(mfrow=c(2,2))
    traceplot(results1)
    ```

    ```{r fig = TRUE} 
    # And the density plots
    par(mfrow=c(2,2))
    densplot(results1)

    ```
    
    </details>

* Fit a non-Bayesian model using `glm()` for comparison. How do the model fits
compare?
    <details><summary>Solution</summary>
    
    ```{r fig = TRUE}
    # Fit model - note similarity with bayesx syntax
    glm.output <- glm(formula = typeFAKE ~ titlehasexcl + title_words + negative,
      data = fakenews,
      family = "binomial")
    # Summarise output
    summary(glm.output)
    # Perform ANOVA on each variable in turn
    drop1(glm.output,test="Chisq")
    ```
    
    </details>
    
# Bayesian Poisson Regression

## Model Formulation

To summarise the model formulation presented in the lecture, given a response
variable $Y_i$ representing the counts occurring from a process with mean 
parameter $\lambda_i$:

* $(Y_i \mid \boldsymbol \lambda_i) \sim\mbox{Po}(\lambda_i),\,\, i.i.d.\,\,, i=1, \ldots, n$
\begin{align*}
 \mbox{log}(\lambda_i) & =\eta_i \nonumber\\
\eta_{i} & =\beta_0+\beta_1 x_{i1}+\ldots+\beta_p x_{ip}=\boldsymbol x_i\boldsymbol \beta\nonumber
\end{align*}
assuming the log link function and with linear predictor $\eta_{i}$.

The following example considers a slightly more complex relationship.

## Example: Salmonella

For this example we will use the `salmonella` data set, which can be read in
via the code
```{r}
salmonella <- read.csv("..\\data\\salmonella.csv")
```

This data set appeared in Breslow (1984) where Poisson methods were developed
for analysis. The data record the number of colonies of TA98 Salmonella which
are measured for different dosage levels of quinoline (in `mg` per plate) for
three replicate plates for each dosage level.

The number of colonies is the response variable - an integer count;
and we are interested in the relationship between the numbers of
Salmonella colonies and dose. Theory suggests a dose-response curve of the
form
$ y_i = \beta_0 + \beta_1 \log \left(x_i+10\right) + \beta_2x_i $

Thus we have a single explanatory variable (dosage level) which appears twice
in the formula to represent the curved relatioship.

## Fitting Bayesian Poisson Regression Models

Again we can use package `MCMCpack` to fit this form of Bayesian generalised
linear model, this time using the function `MCMCpoisson()`.

If not loaded already, the package must be loaded into R:
```{r echo=FALSE}
library(MCMCpack)
```

The syntax for fitting a Bayesian Poisson Regression Model with one
response variable and two explanatory variables would be
like so:
```{r eval=FALSE}
results2 <- MCMCpoisson(formula = y~x1+x2,
                 data = data.set)
```
In this example, the first explanatory variable ($x_1) is $\log(x_1+10)$, and
this will need to be calculated before fitting the model.

For a Poisson GLM we will be using a log() link function by default.

## Exercises

* Perform an exploratory assessment of the salmonella data set,
particularly how the number of Salmonella colonies varies with
the dosage of quinoline.

    <details><summary>Solution</summary>
    
    ```{r fig = TRUE}
    # Plot the relationship between number of colonies and dose
    plot(salmonella$Dose,salmonella$y)
    ```
    
    </details>

* Fit a Bayesian model using `MCMCpoisson` using the `y` variable (colonies of 
salmonella) as
Poisson response and the dose as covariate. Examine the output; does the
model fit well, and is there any statistical
evidence that dosage level is associated with the number of salmonella
colonies? In particular, do the 95% credible intervals for $\beta_1$ and
$\beta_2$ contain zero?

    <details><summary>Solution</summary>
    
    ```{r fig = TRUE}
    # Fit model
    salmonella$log.Dose10 <- log(salmonella$Dose+10)
    results2 <- MCMCpoisson(formula = y ~ log.Dose10 + Dose,
                            data = salmonella,
                            burnin = 5000, mcmc = 10000, thin=10,
                            beta.start = c(NA,NA,NA), # uses maximum likelihood estimates as starting values when NA
                            b0 = c(0,1,1),
                            B0 = c(0.0001,0.0001,0.0001),
                            verbose=1000)
    # Summarise output
    summary(results2)
    ```
    
    </details>


* Produce plots of the MCMC sample traces and the estimated posterior
distributions for the model parameters. Does it seem like convergence has been
achieved?

    <details><summary>Solution</summary>
    
    ```{r fig = TRUE, fig.width = 7, fig.height = 9}
    # Traces can be obtained separately
    par(mfrow =c(2, 2))
    traceplot(results2)
    ```

    ```{r fig = TRUE, fig.width = 7, fig.height = 7}
    # And the density plots
    par(mfrow =c(2, 2))
    densplot(results2)
    ```
    
    </details>

* Plot the curved relationship from the fitted model, showing how Salmonella
colonies are predicted to vary with dose. Add the 95% prediction intervals;
do the intervals look wide enough, that is, do they capture most of the data
points?
    <details><summary>Solution</summary>
    
    ```{r fig = TRUE}
    plot(salmonella$Dose,salmonella$y,ylab="Number of colonies")
    x.predict <- seq(0,1000,length=1001)
    beta.0 <- summary(results2)[[1]]["(Intercept)","Mean"]
    beta.1 <- summary(results2)[[1]]["log.Dose10","Mean"]
    beta.2 <- summary(results2)[[1]]["Dose","Mean"]
    y.predict <-exp(beta.0 + beta.1 * log(x.predict+10) + beta.2 * x.predict)
    lines(x.predict,y.predict)
    # Now let's plot the prediction intervals - here we need to use the full set of MCMC samples
    x.pred.matrix <- as.matrix(x.predict) # Needed to use apply()
    y.pred.matrix <- apply(x.pred.matrix,1,function(x){results2[,"(Intercept)"]+results2[,"log.Dose10"]*log(x+10)+results2[,"Dose"]*x})
    y.predict.lower <- exp(apply(y.pred.matrix,2,function(x){quantile(x,0.025)}))
    lines(x.predict,y.predict.lower,lty=2)
    x.pred.matrix <- as.matrix(x.predict) # Needed to use apply()
    y.pred.matrix <- apply(x.pred.matrix,1,function(x){results2[,"(Intercept)"]+results2[,"log.Dose10"]*log(x+10)+results2[,"Dose"]*x})
    y.predict.upper <- exp(apply(y.pred.matrix,2,function(x){quantile(x,0.975)}))
    lines(x.predict,y.predict.upper,lty=2)
    ```

    </details>
    
* Fit a non-Bayesian model using `glm()` for comparison. How do the model fits
compare?
    <details><summary>Solution</summary>
    
    ```{r fig = TRUE}
    # Fit model - note similarity with MCMCpack syntax
    glm.output <- glm(formula = y ~ log.Dose10 + Dose,
      data = salmonella,
      family = "poisson")
    # Summarise output
    summary(glm.output)
    # Perform ANOVA on each variable in turn
    drop1(glm.output, test = "Chisq")
    ```

    </details>

## Reference

Breslow, N. E. (1984). Extra-Poisson Variation in Log-Linear Models. Journal of the Royal Statistical Society. Series C (Applied Statistics), 33(1), 38–44. https://doi.org/10.2307/2347661
