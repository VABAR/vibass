---
title: 'Practical 5: Numerical approaches'
author: "VIBASS"
date: "July 2023"
output:
  html_vignette:
    fig_caption: yes
    number_sections: yes
    toc: yes
    fig_width: 6
    fig_height: 4
vignette: >
  %\VignetteIndexEntry{Practical 5: Numerical approaches}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


# Introduction

In previous practicals you have used Bayesian models with conjugate
priors where the posterior distribution can be easily worked out. In
general, this is seldom the case and other approaches need to be
considered. In particular, Importance Sampling and Markov Chain Monte
Carlo (MCMC) methods can be used to draw samples from the
posterior distribution that are in turn used to obtain estimates of the
posterior mean and variance and other quantities of interest.

# Importance Sampling

As described in the previous lecture, Importance Sampling (IS) is an
algorithm to estimate some quantities of interest of a target
distribution by sampling from a different (proposal) distribution and
reweighting the samples using importance weights. In Bayesian inference,
IS can be used to sample from the posterior distribution when the
normalizing constant is not known because
$$
\pi(\theta \mid \mathbf{y}) \propto L(\theta \mid \mathbf{y}) \pi(\theta),
$$
where $\mathbf{y}$ represents the observed data,
$L(\theta \mid \mathbf{y})$ the likelihood function and 
$\pi(\theta)$ the prior distribution on $\theta$.

If $g(\cdot)$ is a proposal distribution, and $\{\theta^{(m)}\}_{m=1}^M$
are $M$ samples from that distribution, then the importance weights are
$$
w_m = \frac{L(\theta^{(m)} \mid \mathbf{y})\,\pi(\theta^{(m)})}
  {g(\theta^{(m)})} .
$$
\noindent
When the normalizing constant in the posterior distribution is not
known, the importance weights are re-scaled to sum to one.

Hence, the posterior mean can be computed as
$$
E\left(\theta \mid \mathbf{y}\right) = \mu = \int \theta\, \pi(\theta \mid \mathbf{y}) \mathop{d\theta}
  \simeq \sum_{m=1}^M \theta^{(m)}\, w_m = \hat{\mu}.
$$
\noindent
Similarly, the posterior variance can be computed as
$$
\mbox{Var}\left(\theta \mid \mathbf{y}\right) = 
\sigma^2 = \int (\theta - \mu)^2\, \pi(\theta \mid \mathbf{y}) \mathop{d\theta}
  \simeq \sum_{m=1}^M (\theta^{(m)})^2\, w_m - \hat{\mu}^2 .
$$



# The Metropolis-Hastings Algorithm

The Metropolis-Hastings (M-H) algorithm is a popular MCMC method to
obtain samples from the posterior distribution of an ensemble of
parameters.
In the examples below we will only consider models with one parameter,
but the M-H algorithm can be used on models with a large number of
parameters.

The M-H algorithm works in a very simple way. At every step of the 
algorithm a new movement is proposed using a *proposal distribution*.
This movement is accepted with a known probability, which implies that
the movement can be rejected so that the algorithm stays at the same
state in the current iteration.

Hence, in order to code the M-H algorithm for a set of parameters
$\theta$ we need to define:

* A function to draw observations from the proposal distribution, given
its current state. This will be denoted by $q(\cdot\mid\cdot)$, so that
the density of a new proposal $\theta^*$ given a current state 
$\theta^{(m)}$ is given by $q(\theta^*\mid\theta^{(m)})$.

From the Bayesian model, we already know:

* A prior distribution on the parameters of interest, i.e.,
$\pi(\theta)$.

* The likelihood of the parameter $\theta$ given the observed data
$\mathbf{y}$, i.e., $L(\theta\mid\mathbf{y})$.

At step $m$, a new value is drawn from $q(\cdot\mid\theta^{(m)})$ and it
is accepted with probability:

$$
\alpha = \min\left\{1,
\frac{L(\theta^*\mid\mathbf{y})\,\pi(\theta^{*})\,q(\theta^{(m)}
\mid\theta^{*})}
{L(\theta^{m}\mid\mathbf{y})\,\pi(\theta^{(m)})\,q(\theta^{*}
\mid\theta^{(m)})}\right\}
$$


If the value is accepted, then the current state is set to the proposed
value, i.e., $\theta^{(m+1)} = \theta^{*}$. 
Otherwise we keep the previous value, so
$\theta^{(m+1)} = \theta^{(m)}$.

# Example: Binomial-Beta Model

The first example that will be considered is based on the data set
collected on the number of red M&M's in tube $i$ with $n_i$ M&M's.
The number of red M&M's obtained depends on the total number of M&M's
extracted and the actual proportion of red ones $\theta$.
Given the proportion $\theta$, the number of red M&M's obtained
follows a Binomial distribution. Because $\theta$ takes values between
0 and 1, a sensible choice for a prior is the Beta distribution.

The model can be stated as follows:
$$
\begin{array}{rcl}
y_i \mid \theta & \sim & \text{Bi}(n_i,\, \theta)\\
\theta & \sim & \text{Be}(\alpha,\, \beta)
\end{array}
$$
to allow for multiple tubes $i$.

In particular, we will consider a vague uniform prior in the $[0,1]$ interval, which corresponds to $\alpha=\beta=1$.

You can use the following data set for this exercise:

```{r eval = TRUE}
data <- data.frame(MMs = c(20, 22, 24), red = c(5, 8, 9))
```

These data reproduce different counts of red M&M's in three different
tubes. Variable `MMs` records the total number of M&M's $n_i$ in tube $i$
and `red` the actual number of red M&M's ($y_i$ in the model).

## Importance sampling

Although the posterior distribution is known in closed form, IS can
be used to estimate the posterior mean and variance. Given that
the parameter $\theta$ is bounded, a uniform distribution in the interval $[0,1]$ will
be used. This is probably not very efficient (as it is likely not
to be close to the actual posterior) but it will provide a straightforward simulation strategy.

```{r}
n_simulations <- 10000
set.seed(12)
theta_sim <- runif(n_simulations)
```

Next, importance weights are computed in two steps. First, the
ratio between the likelihood times the prior and the density of the
proposal distribution is computed.
Secondly, weights are re-scaled to sum to one.

```{r}
# Log-Likelihood (for each value of theta_sim)
loglik_binom <- sapply(theta_sim, function(theta) {
  sum(dbinom(data$red, data$MMs, theta, log = TRUE))
})

# Log-weights: log-lik + log-prior - log-proposal_distribution
log_ww <- loglik_binom + dbeta(theta_sim, 1, 1, log = TRUE) - log(1)

# Re-scale weights to sum up to one
log_ww <- log_ww - max(log_ww)
ww <- exp(log_ww)
ww <- ww / sum(ww)

```


Importance weights can be summarized using a histogram (see below).
The distribution of weights shows that most samples are far from the
regions of high posterior density.


```{r fig = TRUE}
hist(ww, xlab = "Importance weights")
``` 

The posterior mean and variance can be computed as follows:

```{r}
# Posterior mean
post_mean <- sum(theta_sim * ww)
post_mean 

# Posterior variance
post_var <- sum(theta_sim^2 * ww)- post_mean^2
post_var
```


Finally, an estimate of the posterior density
$\pi(\theta \mid \mathbf{y})$ 
of the parameter can be obtained by using *weighted* kernel density
estimation.

\hline
\textbf{Aside: weighted kernel density estimation} Standard kernel
density estimation is a way of producing a non-parametric estimate of
the distribution of a continuous quantity given a sample. A ``kernel``
function is selected (typically a Normal density), and one of these is
placed centred on each sample point. The sum of these functions produces
the kernel density estimate (after scaling - dividing by the number of
sample points). A *weighted* kernel density estimate simply includes
weights in the sum of the kernel functions. In both weighted and
unweighted forms of kernel density estimation, the key parameter
controlling the smoothness of the resulting density estimate is the
*bandwidth* (equivalent to the standard deviation if using a Normal
kernel function); larger values give smoother density estimates, and
smaller values give ``noisier`` densities.
\hline

```{r fig = TRUE}
plot(density(theta_sim, weights = ww, bw = 0.01),
     main = "Posterior density")
curve(dbeta(x, sum(data$red) + 1, sum(data$MMs) -sum(data$red) + 1),
      lty = 2, add = TRUE)
```

Note that the value of the bandwidth used (argument `bw`) has been
set manually so that both curves will match (as the default bandwidth
provided a slightly different estimate of the posterior distribution)
to enable a fair comparison.

Similarly, a sample from the posterior distribution can be obtained by 
resampling the original values of theta with their corresponding weights.

```{r}
post_theta_sim <- sample(theta_sim, prob = ww, replace = TRUE)
hist(post_theta_sim, freq = FALSE)
curve(dbeta(x, sum(data$red) + 1, sum(data$MMs) -sum(data$red) + 1),
      lty = 2, add = TRUE)
```


## Metropolis-Hastings

As stated above, the implementation of the M-H algorithm requires a
proposal distribution to obtain new values of the parameter $\theta$.
Usually, the proposal distribution is defined so that the proposed
movement depends on the current value. 
However, in this case we will use a uniform
distribution between 0 and 1 as our proposal distribution.

First of all, we will define the proposal distribution, prior
and likelihood of the model:

```{r}
# Proposal distribution: sampling
rq <- function() {
  runif(1)
}

# Proposal distribution: log-density
logdq <- function(x) {
  dunif(x, log = TRUE)
}

# Prior distribution: Beta(1, 1), log-density
logprior <- function(theta) {
  dbeta(theta, 1, 1, log = TRUE)
}

# Log-Likelihood
loglik <- function(y, theta, N) {
   res <- sum(dbinom(y, N, theta, log = TRUE)) 
}
```

Note that all densities and the likelihood are computed on the log-scale.

Next, an implementation of the M-H algorithms is as follows:

```{r}
# Number of iterations
n.iter <- 40500

# Simulations of the parameter
theta <- rep(NA, n.iter)

# Initial value
theta[1] <- 0.5

# Data
y <- data$red
N <- data$MMs

for(i in 2:n.iter) {
  new.theta <- rq()

  # Log-Acceptance probability
  logacc.prob <- loglik(y, new.theta, N) + logprior(new.theta) + logdq(theta[i - 1])
  logacc.prob <- logacc.prob - loglik(y, theta[i - 1], N) - logprior(theta[i - 1]) - 
    logdq(new.theta)
  logacc.prob <- min(0, logacc.prob) # Note that 0 = log(1)

  if(log(runif(1)) < logacc.prob) {
    # Accept
    theta[i] <- new.theta
  } else {
    # Reject
    theta[i] <- theta[i - 1]
  }
}
```


We will remove the first 500 iterations as burn-in, and thin the
simulations to keep one in 10 to reduce autocorrelation.
After that, we will compute summary statistics
and display a density of the simulations.

```{r}
# Remove burn-in
theta <- theta[-c(1:500)]

# Thinning
theta <- theta[seq(1, length(theta), by = 10)]

# Summary statistics
summary(theta)

par(mfrow = c(1, 2))
plot(theta, type = "l", main = "MCMC samples", ylab = expression(theta))
plot(density(theta), main = "Posterior density", xlab = expression(theta))
```

## Exercises


### Performance of the proposal distribution

The proposal distribution plays a crucial role in IS and it should be
as close to the posterior as possible. As a way of measuring how good
a proposal distribution is, it is possible to compute the
*effective* sample size as follows:

$$
\text{ESS} = \frac{(\sum_{m=1}^M w_m)^2}{\sum_{m=1}^M w^2_m}.
$$

* Compute the effective sample size for the previous example.
How is this related to the number of IS samples (`n_simulations`)?

    <details><summary>Solution</summary>
    
    ```{r}
    ESS <- function(ww){
      (sum(ww)^2)/sum(ww^2)
    }
    ESS(ww)
    n_simulations
    ```
    
    </details>


* Use a different proposal distribution and check how sampling weights,
ESS and point estimates differ from those in the current example. For
example, a $\text{Be}(20,\, 10)$ will put more mass on high values of
$\theta$, unlike the actual posterior distribution.
What differences do you find with
the example presented here using a uniform proposal distribution? Why
do you think that these differences appear?


    <details><summary>Solution</summary>
    
    ```{r}
    n_simulations <- 10000
    set.seed(12)
    theta_sim <- rbeta(n_simulations, 20, 10)
    loglik_binom <- sapply(theta_sim, function(theta) {
      sum(dbinom(data$red, data$MMs, theta, log = TRUE))
    })
    log_ww <- loglik_binom + dbeta(theta_sim, 1, 1, log = TRUE) - dbeta(theta_sim, 20, 10, log = TRUE)
    log_ww <- log_ww - max(log_ww)
    ww <- exp(log_ww)
    ww <- ww / sum(ww)
    ```
    
    ```{r fig = TRUE}
    hist(ww, xlab = "Importance weights")
    ```
    
    ```{r}
    (post_mean <- sum(theta_sim * ww))
    (post_var <- sum(theta_sim^2 * ww)- post_mean^2)
    ```
    
    ```{r fig = TRUE}
    plot(density(theta_sim, weights = ww, bw = 0.01), main = "Posterior density")
    curve(dbeta(x, sum(data$red) + 1, sum(data$MMs) -sum(data$red) + 1), lty = 2, add = TRUE)
    ```
    
    ```{r}
    ESS(ww)
    n_simulations
    ```
    
    </details>



### Sampling from the actual posterior

Use the posterior distribution (which for this particular case is known
in a closed form) as the proposal distribution.

* What is the distribution of the importance weights now?
    
    <details><summary>Solution</summary>
    
    ```{r}
    n_simulations <- 10000
    set.seed(12)
    theta_sim <- rbeta(n_simulations, sum(data$red) + 1, sum(data$MMs) -sum(data$red) + 1)
    loglik_binom <- sapply(theta_sim, function(theta) {
      sum(dbinom(data$red, data$MMs, theta, log = TRUE))
    })
    log_ww <- loglik_binom + dbeta(theta_sim, 1, 1, log = TRUE) - dbeta(theta_sim, sum(data$red) + 1, sum(data$MMs) -sum(data$red) + 1, log = TRUE)
    log_ww <- log_ww - max(log_ww)
    ww <- exp(log_ww)
    ww <- ww / sum(ww)
    ```
    
    ```{r fig = TRUE}
    hist(ww, xlab = "Importance weights")
    ```
    
    ```{r}
    (post_mean <- sum(theta_sim * ww))
    (post_var <- sum(theta_sim^2 * ww)- post_mean^2)
    ```
    
    ```{r fig = TRUE}
    plot(density(theta_sim, weights = ww, bw = 0.01), main = "Posterior density")
    curve(dbeta(x, sum(data$red) + 1, sum(data$MMs) -sum(data$red) + 1), lty = 2, add = TRUE)
    ```
    
    </details>


* Compute the effective sample size. How large is it? Why do you think this happens?
    
    <details><summary>Solution</summary>
    
    ```{r}
    ESS(ww)
    n_simulations
    ```
    
    </details>


### Non-conjugate prior

IS and M-H are algorithms that can be used to make inference about
$\theta$ when the posterior density of the parameter is not available
in closed form. This is the case with models which have non-conjugate
priors. As an exercise, try to obtain the posterior density of the same
model with the following non-conjugate prior:

$$
\pi(\theta) \propto (1-\theta)^2,\quad \theta\in[0,1].
$$

This prior has the following shape:

```{r}
curve((1-x)^2, xlab = expression(theta), ylab = NA, yaxt = 'n', bty = 'n')
```

The interpretation of this prior is that smaller values are favoured over
higher values, i.e., our prior information is that the parameter is more
likely to have values close to 0 than values close to 1.

Note that the prior is specified up to a normalising constant (which in
this case is not difficult to compute). However, this constant is not
needed to implement both the IS and M-H algorithms. In the case of IS,
the constant will cancel when the weights are re-scaled to sum to one
and in the case of the M-H algorithm the constant will cancel when the
acceptance ratio is computed.

* Use the IS algorithm to compute and plot the weights using this
non-conjugate distribution, using them to estimate the posterior mean
and variance. Finally, show the posterior density estimate.

    <details><summary>Solution</summary>
    
    ```{r}
    n_simulations <- 10000
    set.seed(12)
    theta_sim <- rbeta(n_simulations,20,10)
    loglik_binom <- sapply(theta_sim, function(theta) {
      sum(dbinom(data$red, data$MMs, theta, log = TRUE))
    })
    log_ww <- loglik_binom + 2*log(1-theta_sim) - dbeta(theta_sim, 20, 10, log = TRUE)
    log_ww <- log_ww - max(log_ww)
    ww <- exp(log_ww)
    ww <- ww / sum(ww)
    ```

    ```{r fig = TRUE}
    hist(ww, xlab = "Importance weights")
    ```
    
    ```{r}
    (post_mean <- sum(theta_sim * ww))
    (post_var <- sum(theta_sim^2 * ww)- post_mean^2)
    ```
    
    ```{r fig = TRUE}
    plot(density(theta_sim, weights = ww, bw = 0.01), main = "Posterior density")
    curve(dbeta(x, sum(data$red) + 1, sum(data$MMs) -sum(data$red) + 1), lty = 2, add = TRUE)
    ```
    
    </details>

* Use the M-H algorithm to produce the posterior density estimate.

    <details><summary>Solution</summary>
    
    ```{r}
    # Proposal distribution: sampling
    rq <- function() {
      runif(1)
    }

    # Proposal distribution: log-density
    logdq <- function(x) {
      dunif(x, log = TRUE)
    }

    # Prior distribution: (1-theta)^2, log-density
    logprior <- function(theta) {
      2*log(1-theta)
    }

    # Log-Likelihood
    loglik <- function(y, theta, N) {
    res <- sum(dbinom(y, N, theta, log = TRUE))
    }
    ```

    ```{r}
    # Number of iterations
    n.iter <- 40500

    # Simulations of the parameter
    theta <- rep(NA, n.iter)

    # Initial value
    theta[1] <- 0.5

    # Data
    y <- data$red
    N <- data$MMs

    for(i in 2:n.iter) {
      new.theta <- rq()

      # Log-Acceptance probability
      logacc.prob <- loglik(y, new.theta, N) + logprior(new.theta) + logdq(theta[i - 1])
      logacc.prob <- logacc.prob - loglik(y, theta[i - 1], N) - logprior(theta[i - 1]) - 
      logdq(new.theta)
      logacc.prob <- min(0, logacc.prob) # Note that 0 = log(1)

      if(log(runif(1)) < logacc.prob) {
        # Accept
        theta[i] <- new.theta
      } else {
        # Reject
        theta[i] <- theta[i - 1]
      }
    }
    ```

    ```{r}
    # Remove burn-in
    theta <- theta[-c(1:500)]

    # Thinning
    theta <- theta[seq(1, length(theta), by = 10)]

    # Summary statistics
    summary(theta)
    ```

    ```{r fig = TRUE}
    par(mfrow = c(1, 2))
    plot(theta, type = "l", main = "MCMC samples", ylab = expression(theta))
    plot(density(theta), main = "Posterior density", xlab = expression(theta))
    ```
    
    </details>

# Example: Poisson-Gamma Model

The second example will be based on the *Game of Thrones* data set.
Remember that this is made of the observed number of u's on
a page of a book of Game of Thrones. The model can be stated as:

$$
\begin{array}{rcl}
y_i \mid \lambda & \sim & \text{Po}(\lambda)\\
\lambda & \sim & \text{Ga}(\alpha,\, \beta)
\end{array}
$$

In particular, the prior on $\lambda$ will be a Gamma distribution with
parameters $0.01$ and $0.01$, which is centred at 1 and has a small
precision (i.e., large variance).

We will denote the observed values by `y` in the `R` code. The data collected can be loaded with:

```{r eval = TRUE}
data <- data.frame(Us = c(25, 29, 27, 27, 25, 27, 22, 26, 27, 29, 23,
                          28, 25, 24, 22, 25, 23, 29, 23, 28, 21, 29,
                          28, 23, 28))
y <- data$Us
```


## Importance sampling

Now the parameter of interest is not bounded, so the proposal
distribution needs to be chosen with care. We will use a log-Normal
distribution with mean 3 and standard deviation equal to 0.5 in the log
scale. This will ensure that all the sampled values are positive (because
$\lambda$ cannot take negative values) and that the sample values are
reasonable (i.e., they are not too small or too large). Note that this
proposal distribution has been chosen having in mind the problem at hand
and that it may not work well with other problems.


```{r}
n_simulations <- 10000
set.seed(1)
lambda_sim <- rlnorm(n_simulations, 3, 0.5)
```

Next, importance weights are computed in two steps, as in the previous
example. Note that now the likelihood, prior and proposal distribution
are different from the ones in the Binomial example.

```{r}
# Log-Likelihood (for each value of lambda_sim)
loglik_pois <- sapply(lambda_sim, function(LAMBDA) {
  sum(dpois(data$Us, LAMBDA, log = TRUE))
})

# Log-weights: log-lik + log-prior - log-proposal_distribution
log_ww <- loglik_pois + dgamma(lambda_sim, 0.01, 0.01, log = TRUE) - dlnorm(lambda_sim, 3, 0.5, log = TRUE)

# Re-scale weights to sum up to one
log_ww <- log_ww - max(log_ww)
ww <- exp(log_ww)
ww <- ww / sum(ww)

```


Importance weights can be summarized using a histogram:


```{r fig = TRUE}
hist(ww, xlab = "Importance weights")
```

The posterior mean and variance can be computed as follows:

```{r}
# Posterior mean
(post_mean <- sum(lambda_sim * ww))

# Posterior variance
(post_var <- sum(lambda_sim^2 * ww)- post_mean^2)
```


Finally, an estimate of the posterior density of the parameter can be
obtained by using weighted kernel density estimation:


```{r fig = TRUE}
plot(density(lambda_sim, weights = ww, bw = 0.5) , main = "Posterior density", xlim = c(10,40))
curve(dgamma(x, sum(data$Us) + 0.01, length(data$Us) + 0.01), lty = 2, add = TRUE)
```

Note that the value of the bandwidth used (argument `bw`) has been set
manually so that both curves will match (as the default bandwidth
provided a slightly different estimate of the posterior distribution) to
enable a fair comparison.

Similarly, a sample from the posterior distribution can be obtained by 
resampling the original values of $\theta$ with their corresponding
weights.

```{r}
post_lambda_sim <- sample(lambda_sim, prob = ww, replace = TRUE)
hist(post_lambda_sim, freq = FALSE)
curve(dgamma(x, sum(data$Us) + 0.01, length(data$Us) + 0.01), lty = 2, add = TRUE)
```





## Metropolis-Hastings

Similarly to the previous example, we will set the proposal
distribution, as the model has been fully defined above. 
In this case, the proposal distribution is a log-Normal distribution
centred at the logarithm of the current value with precision $100$.


```{r}
# Proposal distribution: sampling
rq <- function(lambda) {
  rlnorm(1, meanlog = log(lambda), sdlog = sqrt(1 / 100))
}

# Proposal distribution: log-density
logdq <- function(new.lambda, lambda) {
  dlnorm(new.lambda, meanlog = log(lambda), sdlog = sqrt(1 / 100), log = TRUE)
}

# Prior distribution: Ga(0.01, 0.01)
logprior <- function(lambda) {
  dgamma(lambda, 0.01, 0.01, log = TRUE)
}

# LogLikelihood
loglik <- function(y, lambda) {
   res <- sum(dpois(y, lambda, log = TRUE)) 
}
```


With these definitions we can actually use the same implementation of the
M-H that we have used in the previous section.
 
```{r}
# Number of iterations
n.iter <- 40500

# Simulations of the parameter
lambda <- rep(NA, n.iter)

# Initial value
lambda[1] <- 30

for(i in 2:n.iter) {
  new.lambda <- rq(lambda[i - 1])
  
  # Log-Acceptance probability
  logacc.prob <- loglik(y, new.lambda) + logprior(new.lambda) +
    logdq(lambda[i - 1], new.lambda)
  logacc.prob <- logacc.prob - loglik(y, lambda[i - 1]) - logprior(lambda[i - 1]) - 
    logdq(new.lambda, lambda[i - 1])
  logacc.prob <- min(0, logacc.prob)#0 = log(1)
  
  if(log(runif(1)) < logacc.prob) {
    # Accept
    lambda[i] <- new.lambda
  } else {
    # Reject
    lambda[i] <- lambda[i - 1]
  }
}
```


The same burn-in and thinning as in the Beta-Binomial example will be
used. Furthermore, summary statistics and plots will be computed now:


```{r}
# Remove burn-in
lambda <- lambda[-c(1:500)]

# Thinning
lambda <- lambda[seq(1, length(lambda), by = 10)]

# Summary statistics
summary(lambda)

par(mfrow = c(1, 2))
plot(lambda, type = "l", main = "MCMC samples", ylab = expression(lambda))
plot(density(lambda), main = "Posterior density", xlab = expression(lambda))
```


## Exercises


### Performance of the proposal distribution

Similarly to the exercises in the example on Binomial data, you will
now assess the impact of the proposal distribution on the inference
process.


* Compute the effective sample size for the previous example.
How is this related to the number of IS samples (`n_simulations`)?

    <details><summary>Solution</summary>
    
    ```{r}
    ESS(ww)
    n_simulations
    ```
    
    </details>


* Use a different proposal distribution and check how sampling weights,
ESS and point estimates differ from those in the current example. For
example, a $\text{Ga}(5,\, 0.1)$ will put a higher mass on values
around $40$, unlike the actual posterior distribution. What differences
do you find with the example presented here using a uniform proposal
distribution? Why do you think that these differences appear?
    
    <details><summary>Solution</summary>
    
    ```{r}
    n_simulations <- 10000
    set.seed(12)
    lambda_sim <- rgamma(n_simulations,5,0.1)
    loglik_pois <- sapply(lambda_sim, function(LAMBDA) {
      sum(dpois(data$Us, LAMBDA, log = TRUE))
    })
    log_ww <- loglik_pois + dgamma(lambda_sim, 0.01, 0.01, log = TRUE) - dgamma(lambda_sim, 5, 0.1, log=TRUE)
    log_ww <- log_ww - max(log_ww)
    ww <- exp(log_ww)
    ww <- ww / sum(ww)
    ```
    
    ```{r fig = TRUE}
    hist(ww, xlab = "Importance weights")
    ```
    
    ```{r}
    post_mean <- sum(lambda_sim * ww)
    post_mean
    post_var <- sum(lambda_sim^2 * ww)- post_mean^2
    post_var
    ```
    
    ```{r fig = TRUE}
    plot(density(lambda_sim, weights = ww, bw = 0.5), main = "Posterior density", xlim = c(10,40))
    curve(dgamma(x, sum(data$Us) + 0.01, length(data$Us) + 0.01), lty = 2, add = TRUE)
    ```
    
    ```{r}
    ESS(ww)
    n_simulations
    ```
    
    </details>


### Sampling from the actual posterior

Use the posterior distribution (which for this particular case is known
in a closed form) as the proposal distribution.

* What is the distribution of the importance weights now?
    
    <details><summary>Solution</summary>
    
    ```{r}
    n_simulations <- 10000
    set.seed(12)
    lambda_sim <- rgamma(n_simulations, sum(data$Us) + 0.01, length(data$Us) + 0.01)
    loglik_pois <- sapply(lambda_sim, function(LAMBDA) {
      sum(dpois(data$Us, LAMBDA, log = TRUE))
    })
    log_ww <- loglik_pois + dgamma(lambda_sim, 0.01, 0.01, log = TRUE) - dgamma(lambda_sim, sum(data$Us) + 0.01, length(data$Us) + 0.01, log=TRUE)
    log_ww <- log_ww - max(log_ww)
    ww <- exp(log_ww)
    ww <- ww / sum(ww)
    ```
    
    ```{r fig = TRUE}
    hist(ww, xlab = "Importance weights")
    ```
    
    ```{r}
    (post_mean <- sum(lambda_sim * ww))
    (post_var <- sum(lambda_sim^2 * ww)- post_mean^2)
    ```
    
    ```{r fig = TRUE}
    plot(density(lambda_sim, weights = ww, bw = 0.5) , main = "Posterior density")
    curve(dgamma(x, sum(data$Us) + 0.01, length(data$Us) + 0.01), lty = 2, add = TRUE)
    ```
    
    </details>


* Compute the effective sample size. How large is it? Why do you think this happens?

    <details><summary>Solution</summary>
    
    ```{r}
    ESS(ww)
    n_simulations
    ```
    
    </details>



### Non-conjugate prior

Just as in the Binomial example, non-conjugate priors can be used
for the $\lambda$ parameter. In this case, given that $\lambda$ is
positive, care must be taken when choosing the prior. For this exercise,
try to use a log-Normal prior with mean 4 and standard deviation 1.
This will provide a prior of $\lambda$ as seen in the next plot:

```{r}
curve(
  dlnorm(x, 4, 1),
  xlim = c(0, 600), n = 1e3,
  xlab = expression(lambda), ylab = NA,
  yaxt = 'n', bty = 'n'
)
```

Hence, with this prior small values of the average number of u's are
given a higher prior density.

Parameter estimates for a model with this prior can be obtained using IS and M-H algorithms.

* Fit the model with the non-conjugate prior using the IS algorithm.
    
    <details><summary>Solution</summary>
    
    ```{r}
    n_simulations <- 10000
    set.seed(1)
    lambda_sim <- rlnorm(n_simulations, 3, 0.5)
    ```
    
    ```{r}
    # Log-Likelihood (for each value of lambda_sim)
    loglik_pois <- sapply(lambda_sim, function(LAMBDA) {
      sum(dpois(data$Us, LAMBDA, log = TRUE))
    })

    # Log-weights: log-lik + log-prior - log-proposal_distribution
    log_ww <- loglik_pois + dlnorm(lambda_sim, 4, 1, log = TRUE) - dlnorm(lambda_sim, 3, 0.5, log = TRUE)

    # Re-scale weights to sum up to one
    log_ww <- log_ww - max(log_ww)
    ww <- exp(log_ww)
    ww <- ww / sum(ww)
    ```
    
    ```{r fig = TRUE}
    hist(ww, xlab = "Importance weights")
    ```
    
    ```{r}
    # Posterior mean
    (post_mean <- sum(lambda_sim * ww))
    # Posterior variance
    (post_var <- sum(lambda_sim^2 * ww)- post_mean^2)
    ```
    
    ```{r fig = TRUE}
    plot(density(lambda_sim, weights = ww, bw = 0.5) , main = "Posterior density", xlim = c(10,40))
    curve(dgamma(x, sum(data$Us) + 0.01, length(data$Us) + 0.01), lty = 2, add = TRUE)
    ```
    
    ```{r}
    ESS <- function(ww){
      (sum(ww)^2)/sum(ww^2)
    }
    ESS(ww)
    n_simulations
    ```
    
    </details>
    
* Fit the model with the non-conjugate prior using the M-H algorithm.
    
    <details><summary>Solution</summary>

    ```{r}
    # Proposal distribution: sampling
    rq <- function(lambda) {
      rlnorm(1, meanlog = log(lambda), sdlog = sqrt(1 / 100))
    }

    # Proposal distribution: log-density
    logdq <- function(new.lambda, lambda) {
      dlnorm(new.lambda, meanlog = log(lambda), sdlog = sqrt(1 / 100), log = TRUE)
    }

    # Prior distribution: Log Normal(4,1)
    logprior <- function(lambda) {
      dlnorm(lambda_sim, 4, 1, log = TRUE)
    }

    # LogLikelihood
    loglik <- function(y, lambda) {
      res <- sum(dpois(y, lambda, log = TRUE)) 
    }
    ```
    
    ```{r}
    # Number of iterations
    n.iter <- 40500

    # Simulations of the parameter
    lambda <- rep(NA, n.iter)

    # Initial value
    lambda[1] <- 30

    for(i in 2:n.iter) {
      new.lambda <- rq(lambda[i - 1])
  
      # Log-Acceptance probability
      logacc.prob <- loglik(y, new.lambda) + logprior(new.lambda) +
        logdq(lambda[i - 1], new.lambda)
      logacc.prob <- logacc.prob - loglik(y, lambda[i - 1]) - logprior(lambda[i - 1]) - logdq(new.lambda, lambda[i - 1])
      logacc.prob <- min(0, logacc.prob)#0 = log(1)
  
      if(log(runif(1)) < logacc.prob) {
        # Accept
        lambda[i] <- new.lambda
      } else {
        # Reject
        lambda[i] <- lambda[i - 1]
      }
    }
    ```
    
    ```{r}
    # Remove burn-in
    lambda <- lambda[-c(1:500)]

    # Thinning
    lambda <- lambda[seq(1, length(lambda), by = 10)]

    # Summary statistics
    summary(lambda)
    ```
    
    ```{r fig = TRUE}
    par(mfrow = c(1, 2))
    plot(lambda, type = "l", main = "MCMC samples", ylab = expression(lambda))
    plot(density(lambda), main = "Posterior density", xlab = expression(lambda))
    ```
    
    </details>
