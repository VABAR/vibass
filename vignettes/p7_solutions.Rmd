---
title: 'Solutions 7: Generalized linear models'
author: "VIBASS"
date: "July 2022"
output:
  html_vignette:
    fig_caption: yes
    number_sections: yes
    toc: yes
    fig_width: 6
    fig_height: 4
vignette: >
  %\VignetteIndexEntry{Solutions 7: Generalized linear models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


# Introduction

In the previous practical, Bayesian inference for linear models using Importance
Sampling (IS) and the Metropolis-Hastings (M-H) algorithms has been introduced.
In this practical you will be working with generalized linear models (GLMs)
on the binomial and Poisson examples described earlier.
In particular, you will be implementing the IS and M-H algorithms for
Bayesian inference on GLMs. In case you need to refresh yourself on the details about
IS and M-H, please check the lecture notes or Practical 5.


# Example: Binomial-Beta Model

This data set is described in detail in previous practicals; it concerns
the recorded the number of red M&M's in tube $i$ with $n_i$ M&M's for
different tubes.

The model can be stated as follows:

$$
\begin{array}{rcl}
y_i \mid \theta & \sim & Bi(n_i, \theta)\\
\theta & \sim & Be(1, 1)
\end{array}
$$

You can use the following data set for this exercise:

```{r eval = TRUE}
data <- data.frame(MMs = c(20, 22, 24), red = c(5, 8, 9))
```

These data reproduce different counts of red M&M's in three different
tubes, with `MMs`recording the total number of M&M's in the tube
and `red` the number of red ones.

Please check Practical 5 in case you need to remember the details of the
implementation of the IS and M-H algorithms for this particular model.  Also,
you can take the `R` code from Practical 5 and modify it to complete the
exercise above.

## Exercises


### Generalized linear models


The current model can be expressed as a generalized linear model as follows:

$$
\begin{array}{rcl}
y_i & \sim & Bi(\theta, n_i)\\
\textrm{logit}(\theta) & = & \beta_0\\
\beta_0 & \sim & N(0, \tau_{\beta_0} = 0.001)
\end{array}
$$


Note that now the model parameter $\theta$ is linked to a
linear predictor ($\beta_0$ in this case) via the logit link function.
Hence, the model parameter is now $\beta_0$.

This GLM can be easily fitted using IS and M-H by treating $\beta_0$
as the unique model parameter.

### Importance sampling

In order to fit this model using the IS algorithm, values of $\beta_0$
need to be sampled. Given that $\beta_0$ is not bounded, we can use
a Normal distribution with zero mean and precision 0.1. Alternatively,
the mean of the sampling distribution can be set to a more reasonable
value, close to the logit of the average proportions of red
M&Ms in the sample (for example, -1.5).

* Implement the IS algorithm using the two sampling distributions proposed above, i.e., a Normal with zero mean and precision 0.1 and a Normal with mean -1.5 and precision 0.1.

```{r}
n_simulations <- 10000
set.seed(12)
# The two proposed sampling distribution means - uncomment the one you want
beta_0_sim_mean <- 0
#beta_0_sim_mean <- -1.5
beta_0_sim <- rnorm(n_simulations,beta_0_sim_mean,sqrt(1/0.1))

# Log-Likelihood (for each value of theta_sim/beta_0_sim)
loglik_binom <- sapply(beta_0_sim, function(ALPHA) {
  theta_sim <- plogis(ALPHA)
  sum(dbinom(data$red, data$MMs, theta_sim, log = TRUE))
})

# Log-weights: log-lik + log-prior - log-sampling_distr
log_ww <- loglik_binom + dnorm(beta_0_sim, 0, sqrt(1/0.001), log = TRUE) - dnorm(beta_0_sim,beta_0_sim_mean,sqrt(1/0.1),log=TRUE)

# Re-scale weights to sum to one
log_ww <- log_ww - max(log_ww)
ww <- exp(log_ww)
ww <- ww / sum(ww)

```

```{r fig = TRUE}
hist(ww, xlab = "Importance weights")
``` 

* Compute the posterior mean and variance for both sets of results.
Are they similar?

```{r}
# Posterior mean
post_mean <- sum(beta_0_sim * ww)
post_mean 

# Posterior variance
post_var <- sum(beta_0_sim^2 * ww)- post_mean^2
post_var

# Posterior mean - on theta
theta_sim <- plogis(beta_0_sim)
post_mean <- sum(theta_sim * ww)
post_mean 

# Posterior variance - on theta
post_var <- sum(theta_sim^2 * ww)- post_mean^2
post_var
```

```{r fig = TRUE}
plot(density(beta_0_sim, weights = ww, bw = 0.05), main = "Posterior density for beta_0")
```

```{r fig = TRUE}
plot(density(theta_sim, weights = ww, bw = 0.01), main = "Posterior density for theta")
curve(dbeta(x, sum(data$red) + 1, sum(data$MMs) -sum(data$red) + 1), lty = 2, add = TRUE)
```

* Compute and compare the effective sample sizes obtained with both
sampling distributions. What do you find? Why do you think that this happens?

```{r}
ESS <- function(ww){
  (sum(ww)^2)/sum(ww^2)
}
ESS(ww)
n_simulations
```


### Metropolis-Hastings

Similarly, the previous GLM can be fit using the M-H algorithm. Now, values
of $\beta_0$ are proposed, for which a Normal distribution centered at
the current value and precision 0.1 can be used. However, the precision
value can be tuned if proposed values are rejected too often or not
rejected at all.


* Implement the M-H algorithm for this example.

```{r}
#Proposal distribution: sampling
rq <- function(beta_0) {
  res <- rnorm(1, beta_0, sqrt(1 / 0.1)) #Sample beta_0

  return(res)
}

#Proposal distribution: log-density
logdq <- function(new.beta_0, beta_0) {
  res <- dnorm(new.beta_0, beta_0, sd = sqrt(1 / 0.1), log = TRUE)
}

#Prior distribution
logprior <- function(beta_0) {
  res <- dnorm(beta_0, 0, sd = sqrt(1 / 0.001), log = TRUE) #beta_0

  return(res)
}

#LogLikelihood
loglik <- function(beta_0) {
  theta_sim <- plogis(beta_0)
  sum(dbinom(data$red, data$MMs, theta_sim, log = TRUE))
}
```

```{r}
#Number of iterations
n.iter <- 40500

#Simulations of the parameter
beta_0 <- numeric(n.iter)

#Initial value
beta_0[1] <- 0.0

for(i in 2:n.iter) {
  new.beta_0 <- rq(beta_0[i - 1])
  
  #Log-Acceptance probability
  logacc.prob <- loglik(new.beta_0) + logprior(new.beta_0) + logdq(beta_0[i - 1], new.beta_0)
  logacc.prob <- logacc.prob - loglik(beta_0[i - 1]) - logprior(beta_0[i - 1]) - 
    logdq(new.beta_0, beta_0[i - 1])
  logacc.prob <- min(0, logacc.prob)#0 = log(1)
  
  if(log(runif(1)) < logacc.prob) {
    #Accept
    beta_0[i] <- new.beta_0
  } else {
    #Reject
    beta_0[i] <- beta_0[i - 1]
  }
}
```

* Compute the posterior mean and variance of $\beta_0$ and $\theta$.

```{r}
#Remove burn-in
beta_0 <- beta_0[-c(1:500)]

#Thinning
beta_0 <- beta_0[seq(1, length(beta_0), by = 10)]

#Summary statistics
summary(beta_0)

#Summary statistics - on theta
theta <- plogis(beta_0)
summary(theta)

par(mfrow = c(2, 2))
#Alpha
plot(beta_0, type = "l", main = "MCMC samples",
  ylab = expression(beta_0))
plot(density(beta_0), main = "Posterior density",
  xlab = expression(beta_0))
#Theta
plot(theta, type = "l", main = "MCMC samples",
  ylab = expression(theta))
plot(density(theta), main = "Posterior density",
  xlab = expression(theta))
```


* Compare these results to those obtained with the IS algorithm.

The results look very similar - Importance Sampling may be adequate for this simple example, if the sampling distribution is chosen appropriately. Remember that it always possible to tune the sampling and proposal distributions from IS and M-H respectively.

# Example: Poisson-Gamma Model

The second example will be based on the *Game of Thrones* data set, which
has also been described in previous practicals.
Remember that this is made of the observed number of u's on
a page of a book of Game of Thrones. The model can be stated as:

$$
\begin{array}{rcl}
y_i & \sim & Po(\theta)\\
\theta & \sim & Ga(0.01, 0.01)
\end{array}
$$

We will denote the observed values by `y` in the `R` code. The data can be
loaded with:

```{r eval = TRUE}
data <- data.frame(Us = c(25, 29, 27, 27, 25, 27, 22, 26, 27, 29, 23, 28, 25,
  24, 22, 25, 23, 29, 23, 28, 21, 29, 28, 23, 28))
y <- data$Us
```

Again, please check Practical 5 if you need to recall the details of
the implementation of the IS and M-H algorithms for this particular
example. You can also use the `R` code from Practical 5 to develop the following
exercises.

## Exercises

### Generalized linear models


The current model can be expressed as a generalized linear model as follows:

$$
\begin{array}{rcl}
y_i & \sim & Po(\theta)\\
\log(\theta) & = & \beta_0\\
\beta_0 & \sim & N(0, \tau_{\beta_0} = 0.001)
\end{array}
$$


Note that now the model parameter $\theta$ is linked to a
linear predictor ($\beta_0$ in this case) via the logarithm link function.
Hence, the model parameter is now $\beta_0$.

This GLM can be easily fitted using IS and M-H by treating $\beta_0$
as the unique model parameter.

### Importance sampling

In order to fit this model using the IS algorithm, values of $\beta_0$
need to be sampled. Given that $\beta_0$ is not bounded, we can use
a Normal distribution with zero mean and precision 0.1. Alternatively,
the mean of the sampling distribution can be set to a more reasonable
value, close to the logarithm of the average number of u's 
in the sample (for example, 3.25).

* Implement the IS using the two sampling distributions proposed above, i.e.,
a Normal with zero mean and precision 0.1 and a Normal with mean 3.25 and 
precision 0.1.

```{r}
n_simulations <- 10000
set.seed(12)
# The two proposed sampling distribution means - uncomment the one you want
beta_0_sim_mean <- 0
#beta_0_sim_mean <- 3.25
beta_0_sim <- rnorm(n_simulations,beta_0_sim_mean,sqrt(1/0.1))

# Log-Likelihood (for each value of theta_sim/beta_0_sim)
loglik_pois <- sapply(beta_0_sim, function(ALPHA) {
  theta_sim <- exp(ALPHA)
  sum(dpois(y, theta_sim, log = TRUE))
})

# Log-weights: log-lik + log-prior - log-sampling_distr
log_ww <- loglik_pois + dnorm(beta_0_sim, 0.0, sqrt(1/0.001), log = TRUE) - dnorm(beta_0_sim, beta_0_sim_mean, sqrt(1/0.1), log=TRUE)

# Re-scale weights to sum to one
log_ww <- log_ww - max(log_ww)
ww <- exp(log_ww)
ww <- ww / sum(ww)
```

```{r fig = TRUE}
hist(ww, xlab = "Importance weights")
``` 

* Compute the posterior mean and variance for both sets of results.

```{r}
# Posterior mean
post_mean <- sum(beta_0_sim * ww)
post_mean

# Posterior variance
post_var <- sum(beta_0_sim^2 * ww)- post_mean^2
post_var

# Posterior mean - on theta
theta_sim <- exp(beta_0_sim)
post_mean <- sum(theta_sim * ww)
post_mean

# Posterior variance - on theta
post_var <- sum(theta_sim^2 * ww)- post_mean^2
post_var
```

* Compute and compare the effective sample sizes obtained with both
sampling distributions. What do you find? Why do you think that this happens?

```{r}
ESS(ww)
n_simulations
```


### Metropolis-Hastings

Again, the previous GLM can be fitted using the M-H algorithm. Now, values
of $\beta_0$ are proposed, for which a Normal distribution centered at
the current value and precision 0.1 can be used. However, the precision
value can be tuned if proposed values are rejected too often or not
rejected at all.


* Implement the M-H algorithm for this model.

```{r}
#Proposal distribution: sampling
rq <- function(beta_0) {
  res <- rnorm(1, beta_0, sqrt(1 / 0.1)) #Sample beta_0

  return(res)
}

#Proposal distribution: log-density
logdq <- function(new.beta_0, beta_0) {
  res <- dnorm(new.beta_0, beta_0, sd = sqrt(1 / 0.1), log = TRUE)
}

#Prior distribution
logprior <- function(beta_0) {
  res <- dnorm(beta_0, 0, sd = sqrt(1 / 0.001), log = TRUE) #beta_0

  return(res)
}

#LogLikelihood
loglik <- function(y,beta_0) {
  theta_sim <- exp(beta_0)
  sum(dpois(y, theta_sim, log = TRUE))
}
```

```{r}
#Number of iterations
n.iter <- 40500

#Simulations of the parameter
beta_0 <- numeric(n.iter)

#Initial value
beta_0[1] <- 0.0

for(i in 2:n.iter) {
  new.beta_0 <- rq(beta_0[i - 1])
  
  #Log-Acceptance probability
  logacc.prob <- loglik(y,new.beta_0) + logprior(new.beta_0) + logdq(beta_0[i - 1], new.beta_0)
  logacc.prob <- logacc.prob - loglik(y,beta_0[i - 1]) - logprior(beta_0[i - 1]) - 
    logdq(new.beta_0, beta_0[i - 1])
  logacc.prob <- min(0, logacc.prob)#0 = log(1)
  
  if(log(runif(1)) < logacc.prob) {
    #Accept
    beta_0[i] <- new.beta_0
  } else {
    #Reject
    beta_0[i] <- beta_0[i - 1]
  }
}
```

* Compute the posterior mean and variance of $\beta_0$ and $\theta$.

```{r}
#Remove burn-in
beta_0 <- beta_0[-c(1:500)]

#Thinning
beta_0 <- beta_0[seq(1, length(beta_0), by = 10)]

#Summary statistics
summary(beta_0)

#Summary statistics - on theta
theta <- exp(beta_0)
summary(theta)
#Variance of estimate?
var(theta)

par(mfrow = c(2, 2))
#Alpha
plot(beta_0, type = "l", main = "MCMC samples",
  ylab = expression(beta_0))
plot(density(beta_0), main = "Posterior density",
  xlab = expression(beta_0))
#Theta
plot(theta, type = "l", main = "MCMC samples",
  ylab = expression(theta))
plot(density(theta), main = "Posterior density",
  xlab = expression(theta))
```

* Compare the results obtained now with those obtained with the I-S algorithm.

Again similar for this simple example. The variance of the estimate for $\theta$ is a little larger, perhaps because the M-H has explored the posterior distribution more efficiently.

