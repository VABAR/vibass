<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Practical 5: Numerical approaches ‚Ä¢ vibass</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Practical 5: Numerical approaches">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">


    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">vibass</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.0.49</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles

    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/p1.html">Practical 1: Binary data</a>
    </li>
    <li>
      <a href="../articles/p2count.html">Practical 2: Count data</a>
    </li>
    <li>
      <a href="../articles/p2normal.html">Practical 2: Normal data</a>
    </li>
    <li>
      <a href="../articles/p3.html">Practical 3: Bayesian polynomial regression</a>
    </li>
    <li>
      <a href="../articles/p4.html">Practical 4: Simulation-based Bayesian inference</a>
    </li>
    <li>
      <a href="../articles/p5.html">Practical 5: Numerical approaches</a>
    </li>
    <li>
      <a href="../articles/p6.html">Practical 6: Software and GLMs</a>
    </li>
    <li>
      <a href="../articles/p7.html">Practical 7: Bayesian Hierarchical Modelling</a>
    </li>
    <li>
      <a href="../articles/p8.html">Practical 8: Optional Extra and Advanced Material</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/VABAR/vibass/" class="external-link">
    <span class="fab fa-github fa-lg"></span>

  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Practical 5: Numerical approaches</h1>
                        <h4 data-toc-skip class="author">VIBASS</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/VABAR/vibass/blob/main/vignettes/p5.Rmd" class="external-link"><code>vignettes/p5.Rmd</code></a></small>
      <div class="hidden name"><code>p5.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>In previous practicals you have used Bayesian models with conjugate
priors where the posterior distribution can be easily worked out. In
general, this is seldom the case and other approaches need to be
considered. In particular, Importance Sampling and Markov Chain Monte
Carlo (MCMC) methods can be used to draw samples from the posterior
distribution that are in turn used to obtain estimates of the posterior
mean and variance and other quantities of interest.</p>
</div>
<div class="section level2">
<h2 id="importance-sampling">Importance Sampling<a class="anchor" aria-label="anchor" href="#importance-sampling"></a>
</h2>
<p>As described in the previous lecture, Importance Sampling (IS) is an
algorithm to estimate some quantities of interest of a target
distribution by sampling from a different (proposal) distribution and
reweighting the samples using importance weights. In Bayesian inference,
IS can be used to sample from the posterior distribution when the
normalizing constant is not known because
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚à£</mo><mi>ùê≤</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àù</mo><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚à£</mo><mi>ùê≤</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">
\pi(\theta \mid \mathbf{y}) \propto L(\theta \mid \mathbf{y}) \pi(\theta),
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùê≤</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>
represents the observed data,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚à£</mo><mi>ùê≤</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\theta \mid \mathbf{y})</annotation></semantics></math>
the likelihood function and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi(\theta)</annotation></semantics></math>
the prior distribution on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>.</p>
<p>If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚ãÖ</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation></semantics></math>
is a proposal distribution, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">{</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><msubsup><mo stretchy="false" form="postfix">}</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup></mrow><annotation encoding="application/x-tex">\{\theta^{(m)}\}_{m=1}^M</annotation></semantics></math>
are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
samples from that distribution, then the importance weights are
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>‚à£</mo><mi>ùê≤</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
w(\theta^{(m)}) = \frac{L(\theta^{(m)} \mid \mathbf{y})\,\pi(\theta^{(m)})}
  {g(\theta^{(m)})} .
</annotation></semantics></math> When the normalising constant in the
posterior distribution is not known, the importance weights are rescaled
to sum to one. Note that this rescaling is done by the denominator in
the expression at point 2 on slide 12/28 of the Numerical Approaches
slides you have just seen. In practice, rescaling removes the need for
the denominator and simplifies the calculations throughout (we do it
once, rather than every time).</p>
<p>Hence, the posterior mean can be computed as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚à£</mo><mi>ùê≤</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>Œº</mi><mo>=</mo><mo>‚à´</mo><mi>Œ∏</mi><mspace width="0.167em"></mspace><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚à£</mo><mi>ùê≤</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>dŒ∏</mo><mo>‚âÉ</mo><munderover><mo>‚àë</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mspace width="0.167em"></mspace><mi>w</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mover><mi>Œº</mi><mo accent="true">ÃÇ</mo></mover><mi>.</mi></mrow><annotation encoding="application/x-tex">
E\left(\theta \mid \mathbf{y}\right) = \mu = \int \theta\, \pi(\theta \mid \mathbf{y}) \mathop{d\theta}
  \simeq \sum_{m=1}^M \theta^{(m)}\, w(\theta^{(m)}) = \hat{\mu}.
</annotation></semantics></math> Similarly, the posterior variance can
be computed as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚à£</mo><mi>ùê≤</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>œÉ</mi><mn>2</mn></msup><mo>=</mo><mo>‚à´</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚àí</mo><mi>Œº</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mspace width="0.167em"></mspace><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚à£</mo><mi>ùê≤</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>dŒ∏</mo><mo>‚âÉ</mo><munderover><mo>‚àë</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mspace width="0.167em"></mspace><mi>w</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚àí</mo><msup><mover><mi>Œº</mi><mo accent="true">ÃÇ</mo></mover><mn>2</mn></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
\mbox{Var}\left(\theta \mid \mathbf{y}\right) = 
\sigma^2 = \int (\theta - \mu)^2\, \pi(\theta \mid \mathbf{y}) \mathop{d\theta}
  \simeq \sum_{m=1}^M (\theta^{(m)})^2\, w(\theta^{(m)}) - \hat{\mu}^2 .
</annotation></semantics></math></p>
</div>
<div class="section level2">
<h2 id="the-metropolis-hastings-algorithm">The Metropolis-Hastings Algorithm<a class="anchor" aria-label="anchor" href="#the-metropolis-hastings-algorithm"></a>
</h2>
<p>The Metropolis-Hastings (M-H) algorithm is a popular MCMC method to
obtain samples from the posterior distribution of an ensemble of
parameters. In the example below we will only consider models with one
parameter, but the M-H algorithm can be used on models with a large
number of parameters.</p>
<p>The M-H algorithm works in a very simple way. At every step of the
algorithm a new movement is proposed using a <em>proposal
distribution</em>. This movement is accepted with a known probability,
which implies that the movement can be rejected so that the algorithm
stays at the same state in the current iteration.</p>
<p>Hence, in order to code the M-H algorithm for a set of parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
we need to define:</p>
<ul>
<li>A function to draw observations from the proposal distribution,
given its current state. This will be denoted by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚ãÖ</mo><mo>‚à£</mo><mo>‚ãÖ</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">q(\cdot\mid\cdot)</annotation></semantics></math>,
so that the density of a new proposal
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Œ∏</mi><mo>*</mo></msup><annotation encoding="application/x-tex">\theta^*</annotation></semantics></math>
given a current state
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><annotation encoding="application/x-tex">\theta^{(m)}</annotation></semantics></math>
is given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mo>*</mo></msup><mo>‚à£</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">q(\theta^*\mid\theta^{(m)})</annotation></semantics></math>.</li>
</ul>
<p>From the Bayesian model, we already know:</p>
<ul>
<li><p>A prior distribution on the parameters of interest, i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi(\theta)</annotation></semantics></math>.</p></li>
<li><p>The likelihood of the parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
given the observed data
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùê≤</mi><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math>,
i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ∏</mi><mo>‚à£</mo><mi>ùê≤</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\theta\mid\mathbf{y})</annotation></semantics></math>.</p></li>
</ul>
<p>At step
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>,
a new value is drawn from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>‚ãÖ</mo><mo>‚à£</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">q(\cdot\mid\theta^{(m)})</annotation></semantics></math>
and it is accepted with probability:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi><mo>=</mo><mo>min</mo><mrow><mo stretchy="true" form="prefix">{</mo><mn>1</mn><mo>,</mo><mfrac><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mo>*</mo></msup><mo>‚à£</mo><mi>ùê≤</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>q</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>‚à£</mo><msup><mi>Œ∏</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>‚à£</mo><mi>ùê≤</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>q</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mo>*</mo></msup><mo>‚à£</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo stretchy="true" form="postfix">}</mo></mrow></mrow><annotation encoding="application/x-tex">
\alpha = \min\left\{1,
\frac{L(\theta^*\mid\mathbf{y})\,\pi(\theta^{*})\,q(\theta^{(m)}
\mid\theta^{*})}
{L(\theta^{(m)}\mid\mathbf{y})\,\pi(\theta^{(m)})\,q(\theta^{*}
\mid\theta^{(m)})}\right\}
</annotation></semantics></math></p>
<p>If the value is accepted, then the current state is set to the
proposed value, i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>Œ∏</mi><mo>*</mo></msup></mrow><annotation encoding="application/x-tex">\theta^{(m+1)} = \theta^{*}</annotation></semantics></math>.
Otherwise we keep the previous value, so
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\theta^{(m+1)} = \theta^{(m)}</annotation></semantics></math>.</p>
</div>
<div class="section level2">
<h2 id="example-poisson-gamma-model">Example: Poisson-Gamma Model<a class="anchor" aria-label="anchor" href="#example-poisson-gamma-model"></a>
</h2>
<p>The first example will be based on the <em>Game of Thrones</em> data
set. Remember that this is made of the observed number of u‚Äôs on a page
of a book of Game of Thrones. The model can be stated as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>y</mi><mi>i</mi></msub><mo>‚à£</mo><mi>Œª</mi></mtd><mtd columnalign="center" style="text-align: center"><mo>‚àº</mo></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">Po</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œª</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>Œª</mi></mtd><mtd columnalign="center" style="text-align: center"><mo>‚àº</mo></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">Ga</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ±</mi><mo>,</mo><mspace width="0.167em"></mspace><mi>Œ≤</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{array}{rcl}
y_i \mid \lambda &amp; \sim &amp; \text{Po}(\lambda)\\
\lambda &amp; \sim &amp; \text{Ga}(\alpha,\, \beta)
\end{array}
</annotation></semantics></math></p>
<p>In particular, the prior on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
will be a Gamma distribution with parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0.01</mn><annotation encoding="application/x-tex">0.01</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0.01</mn><annotation encoding="application/x-tex">0.01</annotation></semantics></math>,
which is centred at 1 and has a small precision (i.e., large
variance).</p>
<p>We will denote the observed values by <code>y</code> in the
<code>R</code> code. The data collected can be loaded with:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">GoTdata</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>Us <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">25</span>, <span class="fl">29</span>, <span class="fl">27</span>, <span class="fl">27</span>, <span class="fl">25</span>, <span class="fl">27</span>, <span class="fl">22</span>, <span class="fl">26</span>, <span class="fl">27</span>, <span class="fl">29</span>, <span class="fl">23</span>,</span>
<span>                          <span class="fl">28</span>, <span class="fl">25</span>, <span class="fl">24</span>, <span class="fl">22</span>, <span class="fl">25</span>, <span class="fl">23</span>, <span class="fl">29</span>, <span class="fl">23</span>, <span class="fl">28</span>, <span class="fl">21</span>, <span class="fl">29</span>,</span>
<span>                          <span class="fl">28</span>, <span class="fl">23</span>, <span class="fl">28</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">GoTdata</span><span class="op">$</span><span class="va">Us</span></span></code></pre></div>
<div class="section level3">
<h3 id="importance-sampling-1">Importance sampling<a class="anchor" aria-label="anchor" href="#importance-sampling-1"></a>
</h3>
<p>Now the parameter of interest is not bounded, so the proposal
distribution needs to be chosen with care. We will use a log-Normal
distribution with mean 3 and standard deviation equal to 0.5 in the log
scale. This will ensure that all the sampled values are positive
(because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
cannot take negative values) and that the sample values are reasonable
(i.e., they are not too small or too large). Note that this proposal
distribution has been chosen having in mind the problem at hand and that
it may not work well with other problems.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n_simulations</span> <span class="op">&lt;-</span> <span class="fl">10000</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">lambda_sim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Lognormal.html" class="external-link">rlnorm</a></span><span class="op">(</span><span class="va">n_simulations</span>, <span class="fl">3</span>, <span class="fl">0.5</span><span class="op">)</span></span></code></pre></div>
<p>Next, importance weights are computed in two steps. First, the ratio
between the likelihood times the prior and the density of the proposal
distribution is computed. Secondly, weights are re-scaled to sum to
one.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Log-Likelihood (for each value of lambda_sim)</span></span>
<span><span class="va">loglik_pois</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">sapply</a></span><span class="op">(</span><span class="va">lambda_sim</span>, <span class="kw">function</span><span class="op">(</span><span class="va">LAMBDA</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html" class="external-link">dpois</a></span><span class="op">(</span><span class="va">GoTdata</span><span class="op">$</span><span class="va">Us</span>, <span class="va">LAMBDA</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Log-weights: log-lik + log-prior - log-proposal_distribution</span></span>
<span><span class="va">log_ww</span> <span class="op">&lt;-</span> <span class="va">loglik_pois</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/GammaDist.html" class="external-link">dgamma</a></span><span class="op">(</span><span class="va">lambda_sim</span>, <span class="fl">0.01</span>, <span class="fl">0.01</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Lognormal.html" class="external-link">dlnorm</a></span><span class="op">(</span><span class="va">lambda_sim</span>, <span class="fl">3</span>, <span class="fl">0.5</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Re-scale weights to sum up to one</span></span>
<span><span class="va">log_ww</span> <span class="op">&lt;-</span> <span class="va">log_ww</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">log_ww</span><span class="op">)</span></span>
<span><span class="va">ww</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">log_ww</span><span class="op">)</span></span>
<span><span class="va">ww</span> <span class="op">&lt;-</span> <span class="va">ww</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">ww</span><span class="op">)</span></span></code></pre></div>
<p>The importance weights can be summarized using a histogram:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html" class="external-link">hist</a></span><span class="op">(</span><span class="va">ww</span>, xlab <span class="op">=</span> <span class="st">"Importance weights"</span><span class="op">)</span></span></code></pre></div>
<p><img src="p5_files/figure-html/unnamed-chunk-4-1.png" width="700"></p>
<p>The posterior mean and variance can be computed as follows:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Posterior mean</span></span>
<span><span class="op">(</span><span class="va">post_mean</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">lambda_sim</span> <span class="op">*</span> <span class="va">ww</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 25.71561</span></span></code></pre>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Posterior variance</span></span>
<span><span class="op">(</span><span class="va">post_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">lambda_sim</span><span class="op">^</span><span class="fl">2</span> <span class="op">*</span> <span class="va">ww</span><span class="op">)</span><span class="op">-</span> <span class="va">post_mean</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 0.9987383</span></span></code></pre>
<p>Finally, an estimate of the posterior density of the parameter can be
obtained by using <em>weighted</em> kernel density estimation.</p>
<p><strong>Aside: weighted kernel density estimation</strong> Standard
kernel density estimation is a way of producing a non-parametric
estimate of the distribution of a continuous quantity given a sample. A
<em>kernel</em> function is selected (typically a Normal density), and
one of these is placed centred on each sample point. The sum of these
functions produces the kernel density estimate (after scaling - dividing
by the number of sample points). A <em>weighted</em> kernel density
estimate simply includes weights in the sum of the kernel functions. In
both weighted and unweighted forms of kernel density estimation, the key
parameter controlling the smoothness of the resulting density estimate
is the <em>bandwidth</em> (equivalent to the standard deviation if using
a Normal kernel function); larger values give smoother density
estimates, and smaller values give <em>noisier</em> densities. </p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html" class="external-link">density</a></span><span class="op">(</span><span class="va">lambda_sim</span>, weights <span class="op">=</span> <span class="va">ww</span>, bw <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> , main <span class="op">=</span> <span class="st">"Posterior density"</span>, xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">10</span>,<span class="fl">40</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="p5_files/figure-html/unnamed-chunk-6-1.png" width="700"></p>
<p>Note that the value of the bandwidth used (argument <code>bw</code>)
has been set manually to provide a realistically smooth density
function.</p>
<p>Similarly, a sample from the posterior distribution can be obtained
by resampling the original values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
with their corresponding weights.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">post_lambda_sim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="va">lambda_sim</span>, prob <span class="op">=</span> <span class="va">ww</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html" class="external-link">hist</a></span><span class="op">(</span><span class="va">post_lambda_sim</span>, freq <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="p5_files/figure-html/unnamed-chunk-7-1.png" width="700"></p>
</div>
<div class="section level3">
<h3 id="metropolis-hastings">Metropolis-Hastings<a class="anchor" aria-label="anchor" href="#metropolis-hastings"></a>
</h3>
<p>As stated above, the implementation of the M-H algorithm requires a
proposal distribution to obtain new values of the parameter
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>.
Usually, the proposal distribution is defined so that the proposed
movement depends on the current value. However, in this case the
proposal distribution is a log-Normal distribution centred at the
logarithm of the current value with precision
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math>.</p>
<p>First of all, we will define the proposal distribution, prior and
likelihood of the model:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Proposal distribution: sampling</span></span>
<span><span class="va">rq</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/Lognormal.html" class="external-link">rlnorm</a></span><span class="op">(</span><span class="fl">1</span>, meanlog <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span>, sdlog <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="fl">100</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Proposal distribution: log-density</span></span>
<span><span class="va">logdq</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">new.lambda</span>, <span class="va">lambda</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/Lognormal.html" class="external-link">dlnorm</a></span><span class="op">(</span><span class="va">new.lambda</span>, meanlog <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span>, sdlog <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="fl">100</span><span class="op">)</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Prior distribution: Ga(0.01, 0.01)</span></span>
<span><span class="va">logprior</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/GammaDist.html" class="external-link">dgamma</a></span><span class="op">(</span><span class="va">lambda</span>, <span class="fl">0.01</span>, <span class="fl">0.01</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># LogLikelihood</span></span>
<span><span class="va">loglik</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">y</span>, <span class="va">lambda</span><span class="op">)</span> <span class="op">{</span></span>
<span>   <span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html" class="external-link">dpois</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">lambda</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span> </span>
<span><span class="op">}</span></span></code></pre></div>
<p>Note that all densities and the likelihood are computed on the
log-scale.</p>
<p>Next, an implementation of the M-H algorithm is as follows:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Number of iterations</span></span>
<span><span class="va">n.iter</span> <span class="op">&lt;-</span> <span class="fl">40500</span></span>
<span></span>
<span><span class="co"># Simulations of the parameter</span></span>
<span><span class="va">lambda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="va">n.iter</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Initial value</span></span>
<span><span class="va">lambda</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">30</span></span>
<span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="va">n.iter</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">new.lambda</span> <span class="op">&lt;-</span> <span class="fu">rq</span><span class="op">(</span><span class="va">lambda</span><span class="op">[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Log-Acceptance probability</span></span>
<span>  <span class="va">logacc.prob</span> <span class="op">&lt;-</span> <span class="fu">loglik</span><span class="op">(</span><span class="va">y</span>, <span class="va">new.lambda</span><span class="op">)</span> <span class="op">+</span> <span class="fu">logprior</span><span class="op">(</span><span class="va">new.lambda</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">logdq</span><span class="op">(</span><span class="va">lambda</span><span class="op">[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span>, <span class="va">new.lambda</span><span class="op">)</span></span>
<span>  <span class="va">logacc.prob</span> <span class="op">&lt;-</span> <span class="va">logacc.prob</span> <span class="op">-</span> <span class="fu">loglik</span><span class="op">(</span><span class="va">y</span>, <span class="va">lambda</span><span class="op">[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">-</span> <span class="fu">logprior</span><span class="op">(</span><span class="va">lambda</span><span class="op">[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">-</span> </span>
<span>    <span class="fu">logdq</span><span class="op">(</span><span class="va">new.lambda</span>, <span class="va">lambda</span><span class="op">[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">logacc.prob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">min</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">logacc.prob</span><span class="op">)</span><span class="co">#0 = log(1)</span></span>
<span>  </span>
<span>  <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">&lt;</span> <span class="va">logacc.prob</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="co"># Accept</span></span>
<span>    <span class="va">lambda</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">new.lambda</span></span>
<span>  <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>    <span class="co"># Reject</span></span>
<span>    <span class="va">lambda</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">lambda</span><span class="op">[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>The simulations we have generated are not independent of one another;
each is dependent on the previous one. This has two consequences: the
chain is dependent on the initial, starting value of the parameter(s);
and the sampling chain itself will exhibit <em>autocorrelation</em>.</p>
<p>For this reason, we will remove the first 500 iterations to reduce
the dependence of the sampling on the starting value; and we will keep
only every 10<sup>th</sup> simulation to reduce the autocorrelation in
the sampled series. The 500 iterations we discard are known as the
<strong>burn-in</strong> sample, and the process of keeping only every
10<sup>th</sup> value is called <strong>thinning</strong>.</p>
<p>After that, we will compute summary statistics and display a density
of the simulations.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Remove burn-in</span></span>
<span><span class="va">lambda</span> <span class="op">&lt;-</span> <span class="va">lambda</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">500</span><span class="op">)</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Thinning</span></span>
<span><span class="va">lambda</span> <span class="op">&lt;-</span> <span class="va">lambda</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span>, by <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Summary statistics</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span><span class="co">##   22.55   25.05   25.70   25.73   26.40   29.65</span></span></code></pre>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html" class="external-link">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">lambda</span>, type <span class="op">=</span> <span class="st">"l"</span>, main <span class="op">=</span> <span class="st">"MCMC samples"</span>, ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html" class="external-link">expression</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html" class="external-link">density</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span>, main <span class="op">=</span> <span class="st">"Posterior density"</span>, xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html" class="external-link">expression</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="p5_files/figure-html/unnamed-chunk-10-1.png" width="700"></p>
</div>
<div class="section level3">
<h3 id="exercises">Exercises<a class="anchor" aria-label="anchor" href="#exercises"></a>
</h3>
<div class="section level4">
<h4 id="performance-of-the-proposal-distribution">Performance of the proposal distribution<a class="anchor" aria-label="anchor" href="#performance-of-the-proposal-distribution"></a>
</h4>
<p>The proposal distribution plays a crucial role in IS and it should be
as close to the posterior as possible. As a way of measuring how good a
proposal distribution is, it is possible to compute the
<em>effective</em> sample size as follows:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">ESS</mtext><mo>=</mo><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><munderover><mo>‚àë</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mi>w</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><munderover><mo>‚àë</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mi>w</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Œ∏</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
\text{ESS} = \frac{(\sum_{m=1}^M w(\theta^{(m)}))^2}{\sum_{m=1}^M w(\theta^{(m)})^2}.
</annotation></semantics></math></p>
<ul>
<li>
<p>Compute the effective sample size for the previous example. How
is this related to the number of IS samples
(<code>n_simulations</code>)?</p>
<details><summary><p>Solution</p>
</summary><div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ESS</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">ww</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">ww</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">ww</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/LaplacesDemon/man/ESS.html" class="external-link">ESS</a></span><span class="op">(</span><span class="va">ww</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 941.928</span></span></code></pre>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n_simulations</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 10000</span></span></code></pre>
</details>
</li>
</ul>
</div>
<div class="section level4">
<h4 id="changing-the-proposal-distribution---importance-sampling">Changing the proposal distribution - Importance Sampling<a class="anchor" aria-label="anchor" href="#changing-the-proposal-distribution---importance-sampling"></a>
</h4>
<ul>
<li>
<p>Use a different proposal distribution and check how sampling
weights, ESS and point estimates differ from those in the current
example when using Importance Sampling. For example, a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Ga</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>5</mn><mo>,</mo><mspace width="0.167em"></mspace><mn>0.1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Ga}(5,\, 0.1)</annotation></semantics></math>
will put a higher mass on values around 40, unlike the actual posterior
distribution. What differences do you find with the example presented
here using a uniform proposal distribution? Why do you think that these
differences appear?</p>
<details><summary><p>Solution</p>
</summary><div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n_simulations</span> <span class="op">&lt;-</span> <span class="fl">10000</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">12</span><span class="op">)</span></span>
<span><span class="va">lambda_sim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/GammaDist.html" class="external-link">rgamma</a></span><span class="op">(</span><span class="va">n_simulations</span>,<span class="fl">5</span>,<span class="fl">0.1</span><span class="op">)</span></span>
<span><span class="va">loglik_pois</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">sapply</a></span><span class="op">(</span><span class="va">lambda_sim</span>, <span class="kw">function</span><span class="op">(</span><span class="va">LAMBDA</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html" class="external-link">dpois</a></span><span class="op">(</span><span class="va">GoTdata</span><span class="op">$</span><span class="va">Us</span>, <span class="va">LAMBDA</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span><span class="va">log_ww</span> <span class="op">&lt;-</span> <span class="va">loglik_pois</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/GammaDist.html" class="external-link">dgamma</a></span><span class="op">(</span><span class="va">lambda_sim</span>, <span class="fl">0.01</span>, <span class="fl">0.01</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/GammaDist.html" class="external-link">dgamma</a></span><span class="op">(</span><span class="va">lambda_sim</span>, <span class="fl">5</span>, <span class="fl">0.1</span>, log<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">log_ww</span> <span class="op">&lt;-</span> <span class="va">log_ww</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">log_ww</span><span class="op">)</span></span>
<span><span class="va">ww</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">log_ww</span><span class="op">)</span></span>
<span><span class="va">ww</span> <span class="op">&lt;-</span> <span class="va">ww</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">ww</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html" class="external-link">hist</a></span><span class="op">(</span><span class="va">ww</span>, xlab <span class="op">=</span> <span class="st">"Importance weights"</span><span class="op">)</span></span></code></pre></div>
<p><img src="p5_files/figure-html/unnamed-chunk-13-1.png" width="700"></p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">post_mean</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">lambda_sim</span> <span class="op">*</span> <span class="va">ww</span><span class="op">)</span></span>
<span><span class="va">post_mean</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 25.70199</span></span></code></pre>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">post_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">lambda_sim</span><span class="op">^</span><span class="fl">2</span> <span class="op">*</span> <span class="va">ww</span><span class="op">)</span><span class="op">-</span> <span class="va">post_mean</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="va">post_var</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 1.039709</span></span></code></pre>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html" class="external-link">density</a></span><span class="op">(</span><span class="va">lambda_sim</span>, weights <span class="op">=</span> <span class="va">ww</span>, bw <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>, main <span class="op">=</span> <span class="st">"Posterior density"</span>, xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">10</span>,<span class="fl">40</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="p5_files/figure-html/unnamed-chunk-15-1.png" width="700"></p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/LaplacesDemon/man/ESS.html" class="external-link">ESS</a></span><span class="op">(</span><span class="va">ww</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 445.456</span></span></code></pre>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n_simulations</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 10000</span></span></code></pre>
</details>
</li>
</ul>
</div>
<div class="section level4">
<h4 id="changing-the-prior-distribution---metropolis-hastings">Changing the prior distribution - Metropolis-Hastings<a class="anchor" aria-label="anchor" href="#changing-the-prior-distribution---metropolis-hastings"></a>
</h4>
<ul>
<li>
<p>We can also try using a different prior distribution on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>,
and analyse the data using the Metropolis-Hastings algorithm. Run the
example for a prior distribution on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
which is a Gamma distribution with parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1.0</mn><annotation encoding="application/x-tex">1.0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1.0</mn><annotation encoding="application/x-tex">1.0</annotation></semantics></math>,
which is centred at 1 and has a larger precision (i.e., smaller
variance) than before. How does the different prior distribution change
the estimate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œª</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>,
and why?</p>
<details><summary><p>Solution</p>
</summary><div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Prior distribution: Ga(1.0, 1.0)</span></span>
<span><span class="va">logprior</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/GammaDist.html" class="external-link">dgamma</a></span><span class="op">(</span><span class="va">lambda</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co"># Number of iterations</span></span>
<span><span class="va">n.iter</span> <span class="op">&lt;-</span> <span class="fl">40500</span></span>
<span></span>
<span><span class="co"># Simulations of the parameter</span></span>
<span><span class="va">lambda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="va">n.iter</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Initial value</span></span>
<span><span class="va">lambda</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">30</span></span>
<span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="va">n.iter</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">new.lambda</span> <span class="op">&lt;-</span> <span class="fu">rq</span><span class="op">(</span><span class="va">lambda</span><span class="op">[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># Log-Acceptance probability</span></span>
<span>  <span class="va">logacc.prob</span> <span class="op">&lt;-</span> <span class="fu">loglik</span><span class="op">(</span><span class="va">y</span>, <span class="va">new.lambda</span><span class="op">)</span> <span class="op">+</span> <span class="fu">logprior</span><span class="op">(</span><span class="va">new.lambda</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">logdq</span><span class="op">(</span><span class="va">lambda</span><span class="op">[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span>, <span class="va">new.lambda</span><span class="op">)</span></span>
<span>  <span class="va">logacc.prob</span> <span class="op">&lt;-</span> <span class="va">logacc.prob</span> <span class="op">-</span> <span class="fu">loglik</span><span class="op">(</span><span class="va">y</span>, <span class="va">lambda</span><span class="op">[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">-</span> <span class="fu">logprior</span><span class="op">(</span><span class="va">lambda</span><span class="op">[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">-</span> </span>
<span>    <span class="fu">logdq</span><span class="op">(</span><span class="va">new.lambda</span>, <span class="va">lambda</span><span class="op">[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="va">logacc.prob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">min</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">logacc.prob</span><span class="op">)</span><span class="co">#0 = log(1)</span></span>
<span></span>
<span>  <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">&lt;</span> <span class="va">logacc.prob</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="co"># Accept</span></span>
<span>    <span class="va">lambda</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">new.lambda</span></span>
<span>  <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>    <span class="co"># Reject</span></span>
<span>    <span class="va">lambda</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">lambda</span><span class="op">[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span>
<span><span class="co"># Remove burn-in</span></span>
<span><span class="va">lambda</span> <span class="op">&lt;-</span> <span class="va">lambda</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">500</span><span class="op">)</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Thinning</span></span>
<span><span class="va">lambda</span> <span class="op">&lt;-</span> <span class="va">lambda</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span>, by <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Summary statistics</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span><span class="co">##   21.69   24.12   24.76   24.77   25.42   28.48</span></span></code></pre>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html" class="external-link">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">lambda</span>, type <span class="op">=</span> <span class="st">"l"</span>, main <span class="op">=</span> <span class="st">"MCMC samples"</span>, ylab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html" class="external-link">expression</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html" class="external-link">density</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span>, main <span class="op">=</span> <span class="st">"Posterior density"</span>, xlab <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html" class="external-link">expression</a></span><span class="op">(</span><span class="va">lambda</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="p5_files/figure-html/unnamed-chunk-17-1.png" width="700"></p>
</details>
</li>
</ul>
</div>
</div>
</div>
<div class="section level2">
<h2 id="Gibbs">Gibbs Sampling<a class="anchor" aria-label="anchor" href="#Gibbs"></a>
</h2>
<p>As we have seen in the theory session, Gibbs Sampling is an MCMC
method which allows us to estimate one parameter at a time. This is very
useful for models which have lots of parameters, as in a sense it
reduces a very large multidimensional inference problem to a set of
single dimension problems.</p>
<p>To recap, in order to generate a random sample from the joint density
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ùõâ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œ∏</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Œ∏</mi><mn>2</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>Œ∏</mi><mi>D</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g\left(\mathbf{\theta}\right)=
g\left(\theta_1,\theta_2,\ldots,\theta_D\right)</annotation></semantics></math>
for a model with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>
parameters, we use the following algorithm:</p>
<ol style="list-style-type: decimal">
<li>Start with an initial set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ùõâ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msup><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>Œ∏</mi><mn>1</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>Œ∏</mi><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msubsup><mi>Œ∏</mi><mi>D</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{\theta}^{(0)}=
\left(\theta_1^{(0)},\theta_2^{(0)},\ldots,\theta_D^{(0)}\right)</annotation></semantics></math>
</li>
<li>Generate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>Œ∏</mi><mn>1</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\theta_1^{(1)}</annotation></semantics></math>
from the conditional distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mn>1</mn></msub><mo>‚à£</mo><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>Œ∏</mi><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msubsup><mi>Œ∏</mi><mi>D</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\theta_1 \mid \left(\theta_2^{(0)},\ldots,\theta_D^{(0)}\right)</annotation></semantics></math>
</li>
<li>Generate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>Œ∏</mi><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\theta_2^{(1)}</annotation></semantics></math>
from the conditional distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mn>2</mn></msub><mo>‚à£</mo><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>Œ∏</mi><mn>1</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>Œ∏</mi><mn>3</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msubsup><mi>Œ∏</mi><mi>D</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\theta_2 \mid \left(\theta_1^{(1)},\theta_3^{(0)},\ldots,\theta_D^{(0)}\right)</annotation></semantics></math>
</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mspace width="1.0em"></mspace><mspace width="1.0em"></mspace><mi>‚ãØ</mi></mrow><annotation encoding="application/x-tex">\quad\quad\cdots</annotation></semantics></math></li>
<li>Generate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>Œ∏</mi><mi>D</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\theta_D^{(1)}</annotation></semantics></math>
from the conditional distribution
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∏</mi><mi>D</mi></msub><mo>‚à£</mo><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>Œ∏</mi><mn>1</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>Œ∏</mi><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msubsup><mi>Œ∏</mi><mrow><mi>D</mi><mo>‚àí</mo><mn>1</mn></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\theta_D \mid \left(\theta_1^{(1)},\theta_2^{(1)},\ldots,\theta_{D-1}^{(1)}\right)</annotation></semantics></math>
</li>
<li>Iterate from Step 2.</li>
</ol>
<p>As with Metropolis-Hastings, in Gibbs Sampling we typically discard
the initial simulations (the burn-in period), reducing the dependence on
the initial set of parameter values.</p>
<p>As with the other MCMC algorithms, the resulting simulations
approximate a random sample from the posterior distribution.</p>
</div>
<div class="section level2">
<h2 id="example-simple-linear-regression">Example: Simple Linear Regression<a class="anchor" aria-label="anchor" href="#example-simple-linear-regression"></a>
</h2>
<p>We will illustrate the use of Gibbs Sampling on a simple linear
regression model. Recall that we saw yesterday that we can obtain an
analytical solution for a Bayesian linear regression, but that more
complex models require a simulation approach.</p>
<p>The simple linear regression model we will analyse here is a reduced
version of the general linear regression model we saw yesterday:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>=</mo><msub><mi>Œ≤</mi><mn>0</mn></msub><mo>+</mo><msub><mi>Œ≤</mi><mn>1</mn></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><msub><mi>œµ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
Y_i = \beta_0+\beta_1x_i+\epsilon_i
</annotation></semantics></math> for response variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùêò</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Y</mi><mn>2</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>Y</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Y}=\left(Y_1,Y_2,\ldots,Y_n\right)</annotation></semantics></math>,
explanatory variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùê±</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{x}=\left(x_1,x_2,\ldots,x_n\right)</annotation></semantics></math>
and residual vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùõú</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>œµ</mi><mn>1</mn></msub><mo>,</mo><msub><mi>œµ</mi><mn>2</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>œµ</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{\epsilon}=
\left(\epsilon_1,\epsilon_2,\ldots,\epsilon_n\right)</annotation></semantics></math>
for a sample of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\beta_0</annotation></semantics></math>
is the regression intercept,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\beta_1</annotation></semantics></math>
is the regression slope, and where the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œµ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\epsilon_i</annotation></semantics></math>
are independent with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>œµ</mi><mi>i</mi></msub><mo>‚àº</mo><mtext mathvariant="normal">N</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><msup><mi>œÉ</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\epsilon_i\sim \textrm{N}\left(0,\sigma^2\right)</annotation></semantics></math><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚àÄ</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">\forall i=1,2,\ldots,n</annotation></semantics></math>.
For convenience, we refer to the combined set of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêò</mi><annotation encoding="application/x-tex">\mathbf{Y}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùê±</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>
data as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùíü</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation></semantics></math>.
We also define
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>ùê≤</mi><mo accent="true">ÃÇ</mo></mover><annotation encoding="application/x-tex">\hat{\mathbf{y}}</annotation></semantics></math>
to be the fitted response vector (i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>y</mi><mo accent="true">ÃÇ</mo></mover><mi>i</mi></msub><mo>=</mo><msub><mi>Œ≤</mi><mn>0</mn></msub><mo>+</mo><msub><mi>Œ≤</mi><mn>1</mn></msub><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\hat{y}_i = \beta_0 + \beta_1 x_i</annotation></semantics></math>
from the regression equation) using the current values of the parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\beta_0</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\beta_1</annotation></semantics></math>
and precision
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÑ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tau = 1</annotation></semantics></math>
(remember that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÑ</mi><mo>=</mo><mfrac><mn>1</mn><msup><mi>œÉ</mi><mn>2</mn></msup></mfrac></mrow><annotation encoding="application/x-tex">\tau=\frac{1}{\sigma^2}</annotation></semantics></math>)
from the Gibbs Sampling simulations.</p>
<p>For Bayesian inference, it is simpler to work with precisions
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÑ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
rather than with variances
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œÉ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math>.
Given priors
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Ga</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>Œ±</mi><mo>,</mo><mi>Œ≤</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œ≤</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">N</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œº</mi><msub><mi>Œ≤</mi><mn>0</mn></msub></msub><mo>,</mo><msub><mi>œÑ</mi><msub><mi>Œ≤</mi><mn>0</mn></msub></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mspace width="1.0em"></mspace><mtext mathvariant="normal">and</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œ≤</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">N</mtext><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œº</mi><msub><mi>Œ≤</mi><mn>1</mn></msub></msub><mo>,</mo><msub><mi>œÑ</mi><msub><mi>Œ≤</mi><mn>1</mn></msub></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{align*}
\pi(\tau) &amp;= \textrm{Ga}\left(\alpha, \beta\right), \\
\pi(\beta_0) &amp;= \textrm{N}\left(\mu_{\beta_0}, \tau_{\beta_0}\right), \quad\textrm{and} \\
\pi(\beta_1) &amp;= \textrm{N}\left(\mu_{\beta_1}, \tau_{\beta_1}\right).
\end{align*}
</annotation></semantics></math> In the final Practical session later
today (supplied as supplementary or advanced material) you will see this
example analysed by deriving the necessary calculations needed to run
the Gibbs Sampling in R without using a specific package, using the
so-called ‚Äúfull conditional‚Äù distributions - that is, the conditional
distributions referred to in the <a href="#Gibbs">Section on Gibbs
Sampling</a>.</p>
<p>We will use the R package MCMCpack to run the Gibbs Sampling for this
simple example, although we will use more advanced software in
Practicals 6 and 7 for more complex examples.</p>
<p>We will study an problem from ecology, looking at the relationship
between water pollution and mayfly size - the data come from the book
<em>Statistics for Ecologists Using R and Excel 2nd edition</em> by Mark
Gardener (ISBN 9781784271398), see <a href="https://pelagicpublishing.com/products/statistics-for-ecologists-using-r-and-excel-gardener-2nd-edition" class="external-link">the
publisher‚Äôs webpage</a>.</p>
<p>The data are as follows:</p>
<ul>
<li><p><code>length</code> - the length of a mayfly in mm;</p></li>
<li><p><code>BOD</code> - biological oxygen demand in mg of oxygen per
litre, effectively a measure of organic pollution (since more organic
pollution requires more oxygen to break it down).</p></li>
</ul>
<p>The data can be read into R:</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Read in data</span></span>
<span><span class="va">BOD</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">200</span>,<span class="fl">180</span>,<span class="fl">135</span>,<span class="fl">120</span>,<span class="fl">110</span>,<span class="fl">120</span>,<span class="fl">95</span>,<span class="fl">168</span>,<span class="fl">180</span>,<span class="fl">195</span>,<span class="fl">158</span>,<span class="fl">145</span>,<span class="fl">140</span>,<span class="fl">145</span>,<span class="fl">165</span>,<span class="fl">187</span>,</span>
<span>         <span class="fl">190</span>,<span class="fl">157</span>,<span class="fl">90</span>,<span class="fl">235</span>,<span class="fl">200</span>,<span class="fl">55</span>,<span class="fl">87</span>,<span class="fl">97</span>,<span class="fl">95</span><span class="op">)</span></span>
<span><span class="va">mayfly.length</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">20</span>,<span class="fl">21</span>,<span class="fl">22</span>,<span class="fl">23</span>,<span class="fl">21</span>,<span class="fl">20</span>,<span class="fl">19</span>,<span class="fl">16</span>,<span class="fl">15</span>,<span class="fl">14</span>,<span class="fl">21</span>,<span class="fl">21</span>,<span class="fl">21</span>,<span class="fl">20</span>,<span class="fl">19</span>,<span class="fl">18</span>,<span class="fl">17</span>,<span class="fl">19</span>,<span class="fl">21</span>,<span class="fl">13</span>,</span>
<span>            <span class="fl">16</span>,<span class="fl">25</span>,<span class="fl">24</span>,<span class="fl">23</span>,<span class="fl">22</span><span class="op">)</span></span>
<span><span class="co"># Create data frame for the analysis</span></span>
<span><span class="va">Data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>BOD<span class="op">=</span><span class="va">BOD</span>,mayfly.length<span class="op">=</span><span class="va">mayfly.length</span><span class="op">)</span></span></code></pre></div>
<p>The package MCMCpack should be loaded into R:</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://CRAN.R-project.org/package=MCMCpack" class="external-link">MCMCpack</a></span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Loading required package: coda</span></span></code></pre>
<pre><code><span><span class="co">## Loading required package: MASS</span></span></code></pre>
<pre><code><span><span class="co">## ##</span></span>
<span><span class="co">## ## Markov Chain Monte Carlo Package (MCMCpack)</span></span></code></pre>
<pre><code><span><span class="co">## ## Copyright (C) 2003-2024 Andrew D. Martin, Kevin M. Quinn, and Jong Hee Park</span></span></code></pre>
<pre><code><span><span class="co">## ##</span></span>
<span><span class="co">## ## Support provided by the U.S. National Science Foundation</span></span></code></pre>
<pre><code><span><span class="co">## ## (Grants SES-0350646 and SES-0350613)</span></span>
<span><span class="co">## ##</span></span></code></pre>
<p>For this Bayesian Linear Regression example, we will use the
<code><a href="https://rdrr.io/pkg/MCMCpack/man/MCMCregress.html" class="external-link">MCMCregress()</a></code> function; use</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">?</span><span class="va">MCMCregress</span></span></code></pre></div>
<p>to see the help page for <code><a href="https://rdrr.io/pkg/MCMCpack/man/MCMCregress.html" class="external-link">MCMCregress()</a></code>. We specify the
formula just as we would for a non-Bayesian regression using the lm
function in Base R. Conjugate priors are used, with Normal priors for
the regression parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
(with means in <code>b0</code> and precisions in <code>B0</code>) and an
inverse Gamma prior for the residual variance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œÉ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math>;
the latter is equivalent to a Gamma prior for the residual precision
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÑ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>.
The parameters for the prior for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÑ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
can either be set as the shape and scale parameters of the Gamma
distribution (<code>c0/2</code> and <code>d0/2</code> respectively) or
as the mean and variance (<code>sigma.mu</code> and
<code>sigma.var</code>).</p>
<div class="section level3">
<h3 id="exercises-1">Exercises<a class="anchor" aria-label="anchor" href="#exercises-1"></a>
</h3>
<p>We will use Gibbs Sampling to fit a Bayesian Linear Regression model
to the mayfly data. We will use the following prior distributions for
the regression parameters:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>œÑ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">Ga</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œ≤</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">N</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0.0001</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mspace width="1.0em"></mspace><mtext mathvariant="normal">and</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>œÄ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Œ≤</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">N</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0.0001</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{align*}
\pi(\tau) &amp;= \textrm{Ga}\left(1, 1\right), \\
\pi(\beta_0) &amp;= \textrm{N}\left(0, 0.0001\right), \quad\textrm{and} \\
\pi(\beta_1) &amp;= \textrm{N}\left(0, 0.0001\right).
\end{align*}
</annotation></semantics></math></p>
<p>We will set the initial values of both
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
parameters to 1, i.e.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Œ≤</mi><mn>0</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>Œ≤</mi><mn>1</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\beta_0^{(0)}=\beta_1^{(0)}</annotation></semantics></math>.
We do not need to set the initial value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œÑ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
because it is simulated first in the Gibbs Sampling within
<code><a href="https://rdrr.io/pkg/MCMCpack/man/MCMCregress.html" class="external-link">MCMCregress()</a></code>. Note that <code><a href="https://rdrr.io/pkg/MCMCpack/man/MCMCregress.html" class="external-link">MCMCregress()</a></code> reports
summaries of the variance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œÉ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math>,
which is helpful to us.</p>
<div class="section level4">
<h4 id="data-exploration">Data exploration<a class="anchor" aria-label="anchor" href="#data-exploration"></a>
</h4>
<ul>
<li>
<p>Investigate the data to see whether a linear regression model
would be sensible. [Hint: a scatterplot and a correlation coefficient
could be helpful.]</p>
<details><summary><p>Solution</p>
</summary><div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Scatterplot</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">BOD</span>,<span class="va">mayfly.length</span><span class="op">)</span></span></code></pre></div>
<p><img src="p5_files/figure-html/unnamed-chunk-21-1.png" width="700"></p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Correlation with hypothesis test</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.test.html" class="external-link">cor.test</a></span><span class="op">(</span><span class="va">BOD</span>,<span class="va">mayfly.length</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">##  Pearson's product-moment correlation</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## data:  BOD and mayfly.length</span></span>
<span><span class="co">## t = -6.52, df = 23, p-value = 1.185e-06</span></span>
<span><span class="co">## alternative hypothesis: true correlation is not equal to 0</span></span>
<span><span class="co">## 95 percent confidence interval:</span></span>
<span><span class="co">##  -0.9107816 -0.6020516</span></span>
<span><span class="co">## sample estimates:</span></span>
<span><span class="co">##        cor </span></span>
<span><span class="co">## -0.8055507</span></span></code></pre>
</details>
</li>
<li>
<p>Run a frequentist simple linear regression using the
<code>lm</code> function in R; this will be useful for comparison with
our Bayesian analysis.</p>
<details><summary><p>Solution</p>
</summary><div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Linear Regression using lm()</span></span>
<span><span class="va">linreg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mayfly.length</span> <span class="op">~</span> <span class="va">BOD</span>, data <span class="op">=</span> <span class="va">Data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">linreg</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## lm(formula = mayfly.length ~ BOD, data = Data)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residuals:</span></span>
<span><span class="co">##    Min     1Q Median     3Q    Max </span></span>
<span><span class="co">## -3.453 -1.073  0.307  1.105  3.343 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Coefficients:</span></span>
<span><span class="co">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">## (Intercept) 27.697314   1.290822   21.46  &lt; 2e-16 ***</span></span>
<span><span class="co">## BOD         -0.055202   0.008467   -6.52 1.18e-06 ***</span></span>
<span><span class="co">## ---</span></span>
<span><span class="co">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residual standard error: 1.865 on 23 degrees of freedom</span></span>
<span><span class="co">## Multiple R-squared:  0.6489, Adjusted R-squared:  0.6336 </span></span>
<span><span class="co">## F-statistic: 42.51 on 1 and 23 DF,  p-value: 1.185e-06</span></span></code></pre>
</details>
</li>
</ul>
</div>
<div class="section level4">
<h4 id="running-the-gibbs-sampler">Running the Gibbs Sampler<a class="anchor" aria-label="anchor" href="#running-the-gibbs-sampler"></a>
</h4>
<ul>
<li>
<p>Use function <code><a href="https://rdrr.io/pkg/MCMCpack/man/MCMCregress.html" class="external-link">MCMCregress()</a></code> to fit a Bayesian simple
linear regression using <code>mayfly.length</code> as the response
variable. Ensure you have a burn-in period so that the initial
simulations are discarded. You can specify the initial values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ≤</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
using the <code>beta.start</code> argument of
<code><a href="https://rdrr.io/pkg/MCMCpack/man/MCMCregress.html" class="external-link">MCMCregress()</a></code>. The function also has the option
<code>verbose</code>, where e.g.¬†setting a value of 1000 implies the
code will show an update to the screen every 1000 iterations.</p>
<details><summary><p>Solution</p>
</summary><div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Bayesian Linear Regression using a Gibbs Sampler</span></span>
<span></span>
<span><span class="co"># Set the size of the burn-in, the number of iterations of the Gibbs Sampler</span></span>
<span><span class="co"># and the level of thinning</span></span>
<span><span class="va">burnin</span> <span class="op">&lt;-</span> <span class="fl">5000</span></span>
<span><span class="va">mcmc</span> <span class="op">&lt;-</span> <span class="fl">10000</span></span>
<span><span class="va">thin</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span></span>
<span><span class="co"># Obtain the samples</span></span>
<span><span class="va">results1</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MCMCpack/man/MCMCregress.html" class="external-link">MCMCregress</a></span><span class="op">(</span><span class="va">mayfly.length</span><span class="op">~</span><span class="va">BOD</span>,</span>
<span>                         b0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.0</span>,<span class="fl">0.0</span><span class="op">)</span>, B0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.0001</span>,<span class="fl">0.0001</span><span class="op">)</span>,</span>
<span>                         c0 <span class="op">=</span> <span class="fl">2</span>, d0 <span class="op">=</span> <span class="fl">2</span>, <span class="co"># Because the prior is Ga(c0/2,d0/2),</span></span>
<span>                         beta.start <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span>,</span>
<span>                         burnin<span class="op">=</span><span class="va">burnin</span>, mcmc<span class="op">=</span><span class="va">mcmc</span>, thin<span class="op">=</span><span class="va">thin</span>,</span>
<span>                         data<span class="op">=</span><span class="va">Data</span>, verbose<span class="op">=</span><span class="fl">1000</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 1 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##  114.59369</span></span>
<span><span class="co">##   -0.46786</span></span>
<span><span class="co">## sigma2 = 21562.35575</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 1001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   28.57579</span></span>
<span><span class="co">##   -0.06136</span></span>
<span><span class="co">## sigma2 =    4.31350</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 2001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   24.73894</span></span>
<span><span class="co">##   -0.03449</span></span>
<span><span class="co">## sigma2 =    4.65105</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 3001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   28.32000</span></span>
<span><span class="co">##   -0.05961</span></span>
<span><span class="co">## sigma2 =    3.93605</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 4001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   26.97821</span></span>
<span><span class="co">##   -0.04849</span></span>
<span><span class="co">## sigma2 =    4.63895</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 5001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   27.41482</span></span>
<span><span class="co">##   -0.05518</span></span>
<span><span class="co">## sigma2 =    3.56662</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 6001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   31.25295</span></span>
<span><span class="co">##   -0.07519</span></span>
<span><span class="co">## sigma2 =    3.67459</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 7001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   28.68306</span></span>
<span><span class="co">##   -0.05846</span></span>
<span><span class="co">## sigma2 =    2.95033</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 8001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   28.08184</span></span>
<span><span class="co">##   -0.05702</span></span>
<span><span class="co">## sigma2 =    4.51031</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 9001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   26.00227</span></span>
<span><span class="co">##   -0.04466</span></span>
<span><span class="co">## sigma2 =    3.80170</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 10001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   27.77085</span></span>
<span><span class="co">##   -0.05513</span></span>
<span><span class="co">## sigma2 =    3.21633</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 11001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   29.41562</span></span>
<span><span class="co">##   -0.06719</span></span>
<span><span class="co">## sigma2 =    3.80747</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 12001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   29.77778</span></span>
<span><span class="co">##   -0.07300</span></span>
<span><span class="co">## sigma2 =    3.74206</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 13001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   26.66759</span></span>
<span><span class="co">##   -0.04763</span></span>
<span><span class="co">## sigma2 =    3.33029</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 14001 of 15000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   29.38684</span></span>
<span><span class="co">##   -0.06209</span></span>
<span><span class="co">## sigma2 =    3.87291</span></span></code></pre>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">results1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Iterations = 5001:14991</span></span>
<span><span class="co">## Thinning interval = 10 </span></span>
<span><span class="co">## Number of chains = 1 </span></span>
<span><span class="co">## Sample size per chain = 1000 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 1. Empirical mean and standard deviation for each variable,</span></span>
<span><span class="co">##    plus standard error of the mean:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##                 Mean       SD  Naive SE Time-series SE</span></span>
<span><span class="co">## (Intercept) 27.72514 1.346336 0.0425749      0.0425749</span></span>
<span><span class="co">## BOD         -0.05551 0.008833 0.0002793      0.0002793</span></span>
<span><span class="co">## sigma2       3.50903 1.049902 0.0332008      0.0301115</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 2. Quantiles for each variable:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##               2.5%      25%      50%      75%    97.5%</span></span>
<span><span class="co">## (Intercept) 24.936 26.91014 27.76271 28.62318 30.22467</span></span>
<span><span class="co">## BOD         -0.073 -0.06136 -0.05565 -0.04992 -0.03727</span></span>
<span><span class="co">## sigma2       1.979  2.74378  3.37009  4.04642  6.01436</span></span></code></pre>
</details>
</li>
<li>
<p>Use the function <code><a href="https://rdrr.io/pkg/coda/man/traceplot.html" class="external-link">traceplot()</a></code> to view the
autocorrelation in the Gibbs sampling simulation chain. Is there any
visual evidence of autocorrelation, or do the samples look
independent?</p>
<details><summary><p>Solution</p>
</summary><div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html" class="external-link">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/coda/man/traceplot.html" class="external-link">traceplot</a></span><span class="op">(</span><span class="va">results1</span><span class="op">)</span></span></code></pre></div>
<p><img src="p5_files/figure-html/unnamed-chunk-24-1.png" width="700"></p>
</details>
</li>
<li>
<p>Use the function <code><a href="https://rdrr.io/pkg/coda/man/densplot.html" class="external-link">densplot()</a></code> to view the shape of the
posterior densities of each parameter.</p>
<details><summary><p>Solution</p>
</summary><div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html" class="external-link">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/coda/man/densplot.html" class="external-link">densplot</a></span><span class="op">(</span><span class="va">results1</span><span class="op">)</span></span></code></pre></div>
<p><img src="p5_files/figure-html/unnamed-chunk-25-1.png" width="700"></p>
</details>
</li>
<li>
<p>As well as autocorrelation with single parameters, we should also
be concerned about cross-correlations between different parameters -
ideally these correlations would be close to zero, as parameters would
be sampled at least approximately independently from each other. Use the
<code>crosscorr</code> function to see the cross-correlation between
samples from the posterior distribution of the regression intercept and
the coefficient for BOD. Are the values close to zero, or to +1 or
-1?</p>
<details><summary><p>Solution</p>
</summary><div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/coda/man/crosscorr.html" class="external-link">crosscorr</a></span><span class="op">(</span><span class="va">results1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##             (Intercept)         BOD      sigma2</span></span>
<span><span class="co">## (Intercept)  1.00000000 -0.96109141 -0.05564442</span></span>
<span><span class="co">## BOD         -0.96109141  1.00000000  0.04639491</span></span>
<span><span class="co">## sigma2      -0.05564442  0.04639491  1.00000000</span></span></code></pre>
</details>
</li>
<li><p>How do the results compare with the frequentist output?</p></li>
</ul>
</div>
<div class="section level4">
<h4 id="reducing-the-autocorrelation-by-mean-centering-the-covariate">Reducing the autocorrelation by mean-centering the covariate<a class="anchor" aria-label="anchor" href="#reducing-the-autocorrelation-by-mean-centering-the-covariate"></a>
</h4>
<ul>
<li>
<p>One method for reducing cross-correlation between regression
parameters in the sampling chains is to mean centre the covariate(s);
this works because it reduces any dependence between the regression
intercept and slope(s). Do this for the current example, noting that you
will need to make a correction on the estimate of the regression
intercept afterwards.</p>
<details><summary><p>Solution</p>
</summary><div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Mean-centre the x covariate</span></span>
<span><span class="va">DataC</span> <span class="op">&lt;-</span> <span class="va">Data</span></span>
<span><span class="va">meanBOD</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">DataC</span><span class="op">$</span><span class="va">BOD</span><span class="op">)</span></span>
<span><span class="va">DataC</span><span class="op">$</span><span class="va">BOD</span> <span class="op">&lt;-</span> <span class="va">DataC</span><span class="op">$</span><span class="va">BOD</span> <span class="op">-</span> <span class="va">meanBOD</span></span>
<span></span>
<span><span class="co"># Set the size of the burn-in, the number of iterations of the Gibbs Sampler</span></span>
<span><span class="co"># and the level of thinning</span></span>
<span><span class="va">burnin</span> <span class="op">&lt;-</span> <span class="fl">50000</span></span>
<span><span class="va">mcmc</span> <span class="op">&lt;-</span> <span class="fl">10000</span></span>
<span><span class="va">thin</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span></span>
<span><span class="co"># Obtain the samples</span></span>
<span><span class="va">results2</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MCMCpack/man/MCMCregress.html" class="external-link">MCMCregress</a></span><span class="op">(</span><span class="va">mayfly.length</span><span class="op">~</span><span class="va">BOD</span>,</span>
<span>                         b0<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.0</span>,<span class="fl">0.0</span><span class="op">)</span>, B0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.0001</span>,<span class="fl">0.0001</span><span class="op">)</span>,</span>
<span>                         c0 <span class="op">=</span> <span class="fl">2</span>, d0 <span class="op">=</span> <span class="fl">2</span>, <span class="co"># Because the prior is Ga(c0/2,d0/2),</span></span>
<span>                         beta.start <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span>,</span>
<span>                         burnin<span class="op">=</span><span class="va">burnin</span>, mcmc<span class="op">=</span><span class="va">mcmc</span>, thin<span class="op">=</span><span class="va">thin</span>,</span>
<span>                         data<span class="op">=</span><span class="va">DataC</span>, verbose<span class="op">=</span><span class="fl">1000</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 1 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   34.69852</span></span>
<span><span class="co">##    0.11498</span></span>
<span><span class="co">## sigma2 = 2946.59852</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 1001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.89512</span></span>
<span><span class="co">##   -0.05741</span></span>
<span><span class="co">## sigma2 =    4.31326</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 2001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   18.78657</span></span>
<span><span class="co">##   -0.04781</span></span>
<span><span class="co">## sigma2 =    4.65200</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 3001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.82113</span></span>
<span><span class="co">##   -0.05692</span></span>
<span><span class="co">## sigma2 =    3.93707</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 4001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.43363</span></span>
<span><span class="co">##   -0.04760</span></span>
<span><span class="co">## sigma2 =    4.63958</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 5001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.55947</span></span>
<span><span class="co">##   -0.06127</span></span>
<span><span class="co">## sigma2 =    3.56661</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 6001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   20.66838</span></span>
<span><span class="co">##   -0.04711</span></span>
<span><span class="co">## sigma2 =    3.67427</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 7001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.92570</span></span>
<span><span class="co">##   -0.04504</span></span>
<span><span class="co">## sigma2 =    2.95035</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 8001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.75247</span></span>
<span><span class="co">##   -0.05315</span></span>
<span><span class="co">## sigma2 =    4.51008</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 9001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.15149</span></span>
<span><span class="co">##   -0.05554</span></span>
<span><span class="co">## sigma2 =    3.80059</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 10001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.66223</span></span>
<span><span class="co">##   -0.05335</span></span>
<span><span class="co">## sigma2 =    3.21754</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 11001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   20.13758</span></span>
<span><span class="co">##   -0.05936</span></span>
<span><span class="co">## sigma2 =    3.80684</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 12001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   20.24235</span></span>
<span><span class="co">##   -0.07159</span></span>
<span><span class="co">## sigma2 =    3.74353</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 13001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.34348</span></span>
<span><span class="co">##   -0.05139</span></span>
<span><span class="co">## sigma2 =    3.33079</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 14001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   20.12937</span></span>
<span><span class="co">##   -0.04232</span></span>
<span><span class="co">## sigma2 =    3.87363</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 15001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.01540</span></span>
<span><span class="co">##   -0.03953</span></span>
<span><span class="co">## sigma2 =    2.98221</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 16001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.35293</span></span>
<span><span class="co">##   -0.05746</span></span>
<span><span class="co">## sigma2 =    2.90274</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 17001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.37079</span></span>
<span><span class="co">##   -0.06663</span></span>
<span><span class="co">## sigma2 =    2.89264</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 18001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.29063</span></span>
<span><span class="co">##   -0.05267</span></span>
<span><span class="co">## sigma2 =    2.43528</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 19001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.45808</span></span>
<span><span class="co">##   -0.06439</span></span>
<span><span class="co">## sigma2 =    3.20901</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 20001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.71731</span></span>
<span><span class="co">##   -0.04909</span></span>
<span><span class="co">## sigma2 =    3.89850</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 21001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.54603</span></span>
<span><span class="co">##   -0.04340</span></span>
<span><span class="co">## sigma2 =    2.97958</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 22001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.52276</span></span>
<span><span class="co">##   -0.05749</span></span>
<span><span class="co">## sigma2 =    3.17695</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 23001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.52406</span></span>
<span><span class="co">##   -0.05491</span></span>
<span><span class="co">## sigma2 =    3.30296</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 24001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.46015</span></span>
<span><span class="co">##   -0.04384</span></span>
<span><span class="co">## sigma2 =    3.70693</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 25001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.62692</span></span>
<span><span class="co">##   -0.05845</span></span>
<span><span class="co">## sigma2 =    5.25006</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 26001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.13747</span></span>
<span><span class="co">##   -0.05090</span></span>
<span><span class="co">## sigma2 =    3.69566</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 27001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.15052</span></span>
<span><span class="co">##   -0.06892</span></span>
<span><span class="co">## sigma2 =    6.34844</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 28001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.88539</span></span>
<span><span class="co">##   -0.05356</span></span>
<span><span class="co">## sigma2 =    3.13399</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 29001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   18.53229</span></span>
<span><span class="co">##   -0.05721</span></span>
<span><span class="co">## sigma2 =    6.66909</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 30001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.17615</span></span>
<span><span class="co">##   -0.06703</span></span>
<span><span class="co">## sigma2 =    3.69534</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 31001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.51222</span></span>
<span><span class="co">##   -0.06441</span></span>
<span><span class="co">## sigma2 =    3.63189</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 32001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   20.02942</span></span>
<span><span class="co">##   -0.04783</span></span>
<span><span class="co">## sigma2 =    3.93347</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 33001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.22779</span></span>
<span><span class="co">##   -0.06256</span></span>
<span><span class="co">## sigma2 =    4.14724</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 34001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.74958</span></span>
<span><span class="co">##   -0.04529</span></span>
<span><span class="co">## sigma2 =    3.61764</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 35001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.05313</span></span>
<span><span class="co">##   -0.06919</span></span>
<span><span class="co">## sigma2 =    2.25023</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 36001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   20.42649</span></span>
<span><span class="co">##   -0.06709</span></span>
<span><span class="co">## sigma2 =    4.42822</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 37001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   20.01155</span></span>
<span><span class="co">##   -0.04444</span></span>
<span><span class="co">## sigma2 =    3.75874</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 38001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.61044</span></span>
<span><span class="co">##   -0.05992</span></span>
<span><span class="co">## sigma2 =    2.81824</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 39001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   20.18721</span></span>
<span><span class="co">##   -0.05873</span></span>
<span><span class="co">## sigma2 =    4.11590</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 40001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.49300</span></span>
<span><span class="co">##   -0.06064</span></span>
<span><span class="co">## sigma2 =    3.19911</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 41001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   18.80278</span></span>
<span><span class="co">##   -0.06293</span></span>
<span><span class="co">## sigma2 =    4.60514</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 42001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.39229</span></span>
<span><span class="co">##   -0.07741</span></span>
<span><span class="co">## sigma2 =    2.73323</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 43001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.47305</span></span>
<span><span class="co">##   -0.05247</span></span>
<span><span class="co">## sigma2 =    3.11187</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 44001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   20.48476</span></span>
<span><span class="co">##   -0.05306</span></span>
<span><span class="co">## sigma2 =    2.70580</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 45001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.54088</span></span>
<span><span class="co">##   -0.06177</span></span>
<span><span class="co">## sigma2 =    4.52469</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 46001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.14175</span></span>
<span><span class="co">##   -0.06490</span></span>
<span><span class="co">## sigma2 =    4.74373</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 47001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.87775</span></span>
<span><span class="co">##   -0.04737</span></span>
<span><span class="co">## sigma2 =    3.81238</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 48001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   20.50685</span></span>
<span><span class="co">##   -0.04792</span></span>
<span><span class="co">## sigma2 =    5.93746</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 49001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.22369</span></span>
<span><span class="co">##   -0.04882</span></span>
<span><span class="co">## sigma2 =    3.58456</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 50001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.65716</span></span>
<span><span class="co">##   -0.05128</span></span>
<span><span class="co">## sigma2 =    2.28963</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 51001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   18.65504</span></span>
<span><span class="co">##   -0.04345</span></span>
<span><span class="co">## sigma2 =    2.80696</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 52001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.84400</span></span>
<span><span class="co">##   -0.05561</span></span>
<span><span class="co">## sigma2 =    4.19969</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 53001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.33364</span></span>
<span><span class="co">##   -0.05816</span></span>
<span><span class="co">## sigma2 =    5.89947</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 54001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   20.08745</span></span>
<span><span class="co">##   -0.05077</span></span>
<span><span class="co">## sigma2 =    3.83621</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 55001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   20.70882</span></span>
<span><span class="co">##   -0.06036</span></span>
<span><span class="co">## sigma2 =    3.36468</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 56001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.74654</span></span>
<span><span class="co">##   -0.07375</span></span>
<span><span class="co">## sigma2 =    4.55154</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 57001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.35105</span></span>
<span><span class="co">##   -0.04160</span></span>
<span><span class="co">## sigma2 =    4.24165</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 58001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   20.19004</span></span>
<span><span class="co">##   -0.04053</span></span>
<span><span class="co">## sigma2 =    2.55143</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## MCMCregress iteration 59001 of 60000 </span></span>
<span><span class="co">## beta = </span></span>
<span><span class="co">##   19.66103</span></span>
<span><span class="co">##   -0.05363</span></span>
<span><span class="co">## sigma2 =    2.07754</span></span></code></pre>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">results2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Iterations = 50001:59991</span></span>
<span><span class="co">## Thinning interval = 10 </span></span>
<span><span class="co">## Number of chains = 1 </span></span>
<span><span class="co">## Sample size per chain = 1000 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 1. Empirical mean and standard deviation for each variable,</span></span>
<span><span class="co">##    plus standard error of the mean:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##                 Mean      SD  Naive SE Time-series SE</span></span>
<span><span class="co">## (Intercept) 19.61403 0.37599 0.0118898      0.0100889</span></span>
<span><span class="co">## BOD         -0.05478 0.00862 0.0002726      0.0002552</span></span>
<span><span class="co">## sigma2       3.47803 1.07811 0.0340929      0.0340929</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 2. Quantiles for each variable:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##                 2.5%      25%      50%      75%    97.5%</span></span>
<span><span class="co">## (Intercept) 18.84960 19.36318 19.63098 19.86544 20.34886</span></span>
<span><span class="co">## BOD         -0.07067 -0.06099 -0.05491 -0.04897 -0.03778</span></span>
<span><span class="co">## sigma2       2.01223  2.72506  3.27807  3.98491  6.20298</span></span></code></pre>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Correct the effect of the mean-centering on the intercept, using the</span></span>
<span><span class="co"># full set of simulated outputs</span></span>
<span><span class="va">results2.simulations</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">results2</span><span class="op">)</span></span>
<span><span class="va">results2.beta.0</span> <span class="op">&lt;-</span> <span class="va">results2.simulations</span><span class="op">[</span>,<span class="st">"(Intercept)"</span><span class="op">]</span> <span class="op">-</span> <span class="va">meanBOD</span> <span class="op">*</span> <span class="va">results2.simulations</span><span class="op">$</span><span class="va">BOD</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">results2.beta.0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span><span class="co">##   22.78   26.77   27.64   27.61   28.49   31.96</span></span></code></pre>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html" class="external-link">var</a></span><span class="op">(</span><span class="va">results2.beta.0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 1.704337</span></span></code></pre>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html" class="external-link">sd</a></span><span class="op">(</span><span class="va">results2.beta.0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 1.305503</span></span></code></pre>
</details>
</li>
<li>
<p>Look at the trace plots, density plots and cross-correlations as
before. Are there any notable differences when mean-centering the
covariate, especially with regard to the cross-correlations?</p>
<details><summary><p>Solution</p>
</summary><div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html" class="external-link">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/coda/man/traceplot.html" class="external-link">traceplot</a></span><span class="op">(</span><span class="va">results2</span><span class="op">)</span></span></code></pre></div>
<p><img src="p5_files/figure-html/unnamed-chunk-28-1.png" width="700"></p>
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html" class="external-link">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/coda/man/densplot.html" class="external-link">densplot</a></span><span class="op">(</span><span class="va">results2</span><span class="op">)</span></span>
<span><span class="co"># Need to use the Base R kernel density function to look at the corrected </span></span>
<span><span class="co"># Intercept</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html" class="external-link">density</a></span><span class="op">(</span><span class="va">results2.beta.0</span>, bw <span class="op">=</span> <span class="fl">0.3404</span><span class="op">)</span>, xlim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">22</span>,<span class="fl">34</span><span class="op">)</span>,</span>
<span>     main <span class="op">=</span> <span class="st">"Density, corrected Intercept"</span><span class="op">)</span></span></code></pre></div>
<p><img src="p5_files/figure-html/unnamed-chunk-29-1.png" width="700"></p>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/coda/man/crosscorr.html" class="external-link">crosscorr</a></span><span class="op">(</span><span class="va">results2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##              (Intercept)         BOD       sigma2</span></span>
<span><span class="co">## (Intercept)  1.000000000  0.02131006 -0.001004385</span></span>
<span><span class="co">## BOD          0.021310056  1.00000000 -0.019271243</span></span>
<span><span class="co">## sigma2      -0.001004385 -0.01927124  1.000000000</span></span></code></pre>
</details>
</li>
</ul>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by VIBASS7, Facundo Mu√±oz.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

      </footer>
</div>






  </body>
</html>
